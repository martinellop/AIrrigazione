{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skyDataset(Dataset):\n",
    "    def __init__(self, dataset_dim, data_path, label_path):\n",
    "        super().__init__()\n",
    "        self.data = np.memmap(data_path, dtype='float32', mode='r', shape=(dataset_dim, 3, 416, 416))\n",
    "        self.label = np.memmap(label_path, dtype='uint8', mode='r', shape=(dataset_dim,))\n",
    "        self.size = dataset_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        image = torch.from_numpy(np.array(image))\n",
    "\n",
    "        label = torch.tensor(self.label[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(skyModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 3, 4, 4)  # 3 out channel, 4x4 con stride 4\n",
    "        self.conv2 = nn.Conv2d(3, 3, 4, 2)  # idem ma con stride 2\n",
    "        self.pooling = nn.MaxPool2d(3, 2)  # 3x3 con stride 3\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(1875, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def eval_acc(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y_true in data_loader:\n",
    "\n",
    "            y_pred = model(x)\n",
    "            y_pred = torch.squeeze(y_pred)\n",
    "            y_pred = torch.round(y_pred)\n",
    "\n",
    "            total += y_true.size(0)\n",
    "            c = (y_pred == y_true).sum().item()\n",
    "            correct += c\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = os.path.join(\".\",\"dataset\", \"train_data.npm\")\n",
    "train_label = os.path.join(\".\",\"dataset\", \"train_label.npm\")\n",
    "test_data = os.path.join(\".\",\"dataset\", \"test_data.npm\")\n",
    "test_label = os.path.join(\".\",\"dataset\", \"test_label.npm\")\n",
    "\n",
    "train_dataset = skyDataset(3200, train_data, train_label)\n",
    "test_dataset = skyDataset(800, test_data, test_label)\n",
    "\n",
    "dl_train = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "dl_test = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 0.7007284760475159\n",
      "Epoch: 0 Batch: 0\n",
      "Train acc: 0.1534375\n",
      "Test acc: 0.03125\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 0 Batch: 1 Loss: 0.6956275701522827\n",
      "Epoch: 0 Batch: 2 Loss: 0.6935051679611206\n",
      "Epoch: 0 Batch: 3 Loss: 0.6897905468940735\n",
      "Epoch: 0 Batch: 4 Loss: 0.6876078844070435\n",
      "Epoch: 0 Batch: 5 Loss: 0.6849812865257263\n",
      "Epoch: 0 Batch: 6 Loss: 0.6819064617156982\n",
      "Epoch: 0 Batch: 7 Loss: 0.6796293258666992\n",
      "Epoch: 0 Batch: 8 Loss: 0.6782955527305603\n",
      "Epoch: 0 Batch: 9 Loss: 0.6732543110847473\n",
      "Epoch: 0 Batch: 10 Loss: 0.6692911386489868\n",
      "Epoch: 0 Batch: 11 Loss: 0.6606659889221191\n",
      "Epoch: 0 Batch: 12 Loss: 0.6590780019760132\n",
      "Epoch: 0 Batch: 13 Loss: 0.6386867761611938\n",
      "Epoch: 0 Batch: 14 Loss: 0.6154899597167969\n",
      "Epoch: 0 Batch: 15 Loss: 0.5813717246055603\n",
      "Epoch: 0 Batch: 16 Loss: 0.5548110604286194\n",
      "Epoch: 0 Batch: 17 Loss: 0.4462732970714569\n",
      "Epoch: 0 Batch: 18 Loss: 0.4377937316894531\n",
      "Epoch: 0 Batch: 19 Loss: 0.3476307690143585\n",
      "Epoch: 0 Batch: 20 Loss: 0.3645615577697754\n",
      "Epoch: 0 Batch: 21 Loss: 0.32811009883880615\n",
      "Epoch: 0 Batch: 22 Loss: 0.21191000938415527\n",
      "Epoch: 0 Batch: 23 Loss: 0.2297157645225525\n",
      "Epoch: 0 Batch: 24 Loss: 0.2908761203289032\n",
      "Epoch: 0 Batch: 25 Loss: 0.13744884729385376\n",
      "Epoch: 0 Batch: 26 Loss: 0.3468904495239258\n",
      "Epoch: 0 Batch: 27 Loss: 0.4022639989852905\n",
      "Epoch: 0 Batch: 28 Loss: 0.22460955381393433\n",
      "Epoch: 0 Batch: 29 Loss: 0.12818577885627747\n",
      "Epoch: 0 Batch: 30 Loss: 0.33754369616508484\n",
      "Epoch: 0 Batch: 31 Loss: 0.2090664952993393\n",
      "Epoch: 0 Batch: 32 Loss: 0.08725742995738983\n",
      "Epoch: 0 Batch: 33 Loss: 0.13546401262283325\n",
      "Epoch: 0 Batch: 34 Loss: 0.10769446194171906\n",
      "Epoch: 0 Batch: 35 Loss: 0.1105031818151474\n",
      "Epoch: 0 Batch: 36 Loss: 0.18205945193767548\n",
      "Epoch: 0 Batch: 37 Loss: 0.1312374472618103\n",
      "Epoch: 0 Batch: 38 Loss: 0.08247056603431702\n",
      "Epoch: 0 Batch: 39 Loss: 0.20883959531784058\n",
      "Epoch: 0 Batch: 40 Loss: 0.13828949630260468\n",
      "Epoch: 0 Batch: 41 Loss: 0.14656615257263184\n",
      "Epoch: 0 Batch: 42 Loss: 0.11402750015258789\n",
      "Epoch: 0 Batch: 43 Loss: 0.4410594701766968\n",
      "Epoch: 0 Batch: 44 Loss: 0.13507206737995148\n",
      "Epoch: 0 Batch: 45 Loss: 0.20085380971431732\n",
      "Epoch: 0 Batch: 46 Loss: 0.05797500163316727\n",
      "Epoch: 0 Batch: 47 Loss: 0.16300781071186066\n",
      "Epoch: 0 Batch: 48 Loss: 0.3001467287540436\n",
      "Epoch: 0 Batch: 49 Loss: 0.143684983253479\n",
      "Epoch: 0 Batch: 50 Loss: 0.06026431918144226\n",
      "Epoch: 0 Batch: 50\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 0 Batch: 51 Loss: 0.2192194014787674\n",
      "Epoch: 0 Batch: 52 Loss: 0.0973961278796196\n",
      "Epoch: 0 Batch: 53 Loss: 0.20888620615005493\n",
      "Epoch: 0 Batch: 54 Loss: 0.13825082778930664\n",
      "Epoch: 0 Batch: 55 Loss: 0.19465763866901398\n",
      "Epoch: 0 Batch: 56 Loss: 0.09928753972053528\n",
      "Epoch: 0 Batch: 57 Loss: 0.0902165099978447\n",
      "Epoch: 0 Batch: 58 Loss: 0.09823976457118988\n",
      "Epoch: 0 Batch: 59 Loss: 0.26172876358032227\n",
      "Epoch: 0 Batch: 60 Loss: 0.17185735702514648\n",
      "Epoch: 0 Batch: 61 Loss: 0.07592673599720001\n",
      "Epoch: 0 Batch: 62 Loss: 0.18797220289707184\n",
      "Epoch: 0 Batch: 63 Loss: 0.2030920535326004\n",
      "Epoch: 0 Batch: 64 Loss: 0.2117152214050293\n",
      "Epoch: 0 Batch: 65 Loss: 0.126543328166008\n",
      "Epoch: 0 Batch: 66 Loss: 0.2402821034193039\n",
      "Epoch: 0 Batch: 67 Loss: 0.2112313210964203\n",
      "Epoch: 0 Batch: 68 Loss: 0.09902960807085037\n",
      "Epoch: 0 Batch: 69 Loss: 0.14586380124092102\n",
      "Epoch: 0 Batch: 70 Loss: 0.24656814336776733\n",
      "Epoch: 0 Batch: 71 Loss: 0.10663706809282303\n",
      "Epoch: 0 Batch: 72 Loss: 0.20225213468074799\n",
      "Epoch: 0 Batch: 73 Loss: 0.19574865698814392\n",
      "Epoch: 0 Batch: 74 Loss: 0.09126152843236923\n",
      "Epoch: 0 Batch: 75 Loss: 0.09351003915071487\n",
      "Epoch: 0 Batch: 76 Loss: 0.2249106615781784\n",
      "Epoch: 0 Batch: 77 Loss: 0.18685220181941986\n",
      "Epoch: 0 Batch: 78 Loss: 0.16390997171401978\n",
      "Epoch: 0 Batch: 79 Loss: 0.1705106645822525\n",
      "Epoch: 0 Batch: 80 Loss: 0.27819302678108215\n",
      "Epoch: 0 Batch: 81 Loss: 0.11216208338737488\n",
      "Epoch: 0 Batch: 82 Loss: 0.12550856173038483\n",
      "Epoch: 0 Batch: 83 Loss: 0.2358645498752594\n",
      "Epoch: 0 Batch: 84 Loss: 0.08364441990852356\n",
      "Epoch: 0 Batch: 85 Loss: 0.1477406919002533\n",
      "Epoch: 0 Batch: 86 Loss: 0.06688416004180908\n",
      "Epoch: 0 Batch: 87 Loss: 0.31141844391822815\n",
      "Epoch: 0 Batch: 88 Loss: 0.06202306970953941\n",
      "Epoch: 0 Batch: 89 Loss: 0.20307227969169617\n",
      "Epoch: 0 Batch: 90 Loss: 0.13803386688232422\n",
      "Epoch: 0 Batch: 91 Loss: 0.05824797600507736\n",
      "Epoch: 0 Batch: 92 Loss: 0.1889490783214569\n",
      "Epoch: 0 Batch: 93 Loss: 0.25472205877304077\n",
      "Epoch: 0 Batch: 94 Loss: 0.08756447583436966\n",
      "Epoch: 0 Batch: 95 Loss: 0.20463654398918152\n",
      "Epoch: 0 Batch: 96 Loss: 0.08766557276248932\n",
      "Epoch: 0 Batch: 97 Loss: 0.042739737778902054\n",
      "Epoch: 0 Batch: 98 Loss: 0.2670630216598511\n",
      "Epoch: 0 Batch: 99 Loss: 0.11828590929508209\n",
      "Epoch: 1 Batch: 0 Loss: 0.22001923620700836\n",
      "Epoch: 1 Batch: 0\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1 Batch: 1 Loss: 0.06719233095645905\n",
      "Epoch: 1 Batch: 2 Loss: 0.11462261527776718\n",
      "Epoch: 1 Batch: 3 Loss: 0.34697818756103516\n",
      "Epoch: 1 Batch: 4 Loss: 0.190263032913208\n",
      "Epoch: 1 Batch: 5 Loss: 0.06342712044715881\n",
      "Epoch: 1 Batch: 6 Loss: 0.060184840112924576\n",
      "Epoch: 1 Batch: 7 Loss: 0.08498510718345642\n",
      "Epoch: 1 Batch: 8 Loss: 0.03924142196774483\n",
      "Epoch: 1 Batch: 9 Loss: 0.0344000943005085\n",
      "Epoch: 1 Batch: 10 Loss: 0.25113019347190857\n",
      "Epoch: 1 Batch: 11 Loss: 0.1304735392332077\n",
      "Epoch: 1 Batch: 12 Loss: 0.21408513188362122\n",
      "Epoch: 1 Batch: 13 Loss: 0.033503297716379166\n",
      "Epoch: 1 Batch: 14 Loss: 0.14316098392009735\n",
      "Epoch: 1 Batch: 15 Loss: 0.24334387481212616\n",
      "Epoch: 1 Batch: 16 Loss: 0.1076240986585617\n",
      "Epoch: 1 Batch: 17 Loss: 0.1422860324382782\n",
      "Epoch: 1 Batch: 18 Loss: 0.18575242161750793\n",
      "Epoch: 1 Batch: 19 Loss: 0.07758843153715134\n",
      "Epoch: 1 Batch: 20 Loss: 0.12331872433423996\n",
      "Epoch: 1 Batch: 21 Loss: 0.2600939869880676\n",
      "Epoch: 1 Batch: 22 Loss: 0.09784054011106491\n",
      "Epoch: 1 Batch: 23 Loss: 0.13872350752353668\n",
      "Epoch: 1 Batch: 24 Loss: 0.10244376212358475\n",
      "Epoch: 1 Batch: 25 Loss: 0.19282349944114685\n",
      "Epoch: 1 Batch: 26 Loss: 0.16044539213180542\n",
      "Epoch: 1 Batch: 27 Loss: 0.09173297137022018\n",
      "Epoch: 1 Batch: 28 Loss: 0.24799126386642456\n",
      "Epoch: 1 Batch: 29 Loss: 0.10823310911655426\n",
      "Epoch: 1 Batch: 30 Loss: 0.08732704818248749\n",
      "Epoch: 1 Batch: 31 Loss: 0.24498841166496277\n",
      "Epoch: 1 Batch: 32 Loss: 0.22010841965675354\n",
      "Epoch: 1 Batch: 33 Loss: 0.12726520001888275\n",
      "Epoch: 1 Batch: 34 Loss: 0.14767521619796753\n",
      "Epoch: 1 Batch: 35 Loss: 0.21604427695274353\n",
      "Epoch: 1 Batch: 36 Loss: 0.21070054173469543\n",
      "Epoch: 1 Batch: 37 Loss: 0.1497790664434433\n",
      "Epoch: 1 Batch: 38 Loss: 0.13025504350662231\n",
      "Epoch: 1 Batch: 39 Loss: 0.06580238789319992\n",
      "Epoch: 1 Batch: 40 Loss: 0.22182278335094452\n",
      "Epoch: 1 Batch: 41 Loss: 0.1497671902179718\n",
      "Epoch: 1 Batch: 42 Loss: 0.10779355466365814\n",
      "Epoch: 1 Batch: 43 Loss: 0.3353461027145386\n",
      "Epoch: 1 Batch: 44 Loss: 0.1382196694612503\n",
      "Epoch: 1 Batch: 45 Loss: 0.17127439379692078\n",
      "Epoch: 1 Batch: 46 Loss: 0.1635037511587143\n",
      "Epoch: 1 Batch: 47 Loss: 0.12372590601444244\n",
      "Epoch: 1 Batch: 48 Loss: 0.06787697970867157\n",
      "Epoch: 1 Batch: 49 Loss: 0.2673555016517639\n",
      "Epoch: 1 Batch: 50 Loss: 0.14317387342453003\n",
      "Epoch: 1 Batch: 50\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1 Batch: 51 Loss: 0.20732134580612183\n",
      "Epoch: 1 Batch: 52 Loss: 0.14918164908885956\n",
      "Epoch: 1 Batch: 53 Loss: 0.16027891635894775\n",
      "Epoch: 1 Batch: 54 Loss: 0.08442671597003937\n",
      "Epoch: 1 Batch: 55 Loss: 0.11983190476894379\n",
      "Epoch: 1 Batch: 56 Loss: 0.19337889552116394\n",
      "Epoch: 1 Batch: 57 Loss: 0.11280917376279831\n",
      "Epoch: 1 Batch: 58 Loss: 0.13953542709350586\n",
      "Epoch: 1 Batch: 59 Loss: 0.12622669339179993\n",
      "Epoch: 1 Batch: 60 Loss: 0.12132992595434189\n",
      "Epoch: 1 Batch: 61 Loss: 0.11567916721105576\n",
      "Epoch: 1 Batch: 62 Loss: 0.12170115858316422\n",
      "Epoch: 1 Batch: 63 Loss: 0.17016473412513733\n",
      "Epoch: 1 Batch: 64 Loss: 0.0971844270825386\n",
      "Epoch: 1 Batch: 65 Loss: 0.06042061746120453\n",
      "Epoch: 1 Batch: 66 Loss: 0.20529448986053467\n",
      "Epoch: 1 Batch: 67 Loss: 0.11183622479438782\n",
      "Epoch: 1 Batch: 68 Loss: 0.067411869764328\n",
      "Epoch: 1 Batch: 69 Loss: 0.08906055986881256\n",
      "Epoch: 1 Batch: 70 Loss: 0.24532920122146606\n",
      "Epoch: 1 Batch: 71 Loss: 0.05163965001702309\n",
      "Epoch: 1 Batch: 72 Loss: 0.216626837849617\n",
      "Epoch: 1 Batch: 73 Loss: 0.0975480005145073\n",
      "Epoch: 1 Batch: 74 Loss: 0.1890859454870224\n",
      "Epoch: 1 Batch: 75 Loss: 0.1765287220478058\n",
      "Epoch: 1 Batch: 76 Loss: 0.1374969780445099\n",
      "Epoch: 1 Batch: 77 Loss: 0.2530663013458252\n",
      "Epoch: 1 Batch: 78 Loss: 0.12871213257312775\n",
      "Epoch: 1 Batch: 79 Loss: 0.1472216695547104\n",
      "Epoch: 1 Batch: 80 Loss: 0.1602294147014618\n",
      "Epoch: 1 Batch: 81 Loss: 0.1066078469157219\n",
      "Epoch: 1 Batch: 82 Loss: 0.2625919282436371\n",
      "Epoch: 1 Batch: 83 Loss: 0.11969810724258423\n",
      "Epoch: 1 Batch: 84 Loss: 0.24520795047283173\n",
      "Epoch: 1 Batch: 85 Loss: 0.05929534137248993\n",
      "Epoch: 1 Batch: 86 Loss: 0.23670342564582825\n",
      "Epoch: 1 Batch: 87 Loss: 0.2739587724208832\n",
      "Epoch: 1 Batch: 88 Loss: 0.14133179187774658\n",
      "Epoch: 1 Batch: 89 Loss: 0.16707785427570343\n",
      "Epoch: 1 Batch: 90 Loss: 0.05641930550336838\n",
      "Epoch: 1 Batch: 91 Loss: 0.07426901161670685\n",
      "Epoch: 1 Batch: 92 Loss: 0.11515690386295319\n",
      "Epoch: 1 Batch: 93 Loss: 0.09848146140575409\n",
      "Epoch: 1 Batch: 94 Loss: 0.1111358106136322\n",
      "Epoch: 1 Batch: 95 Loss: 0.05894600600004196\n",
      "Epoch: 1 Batch: 96 Loss: 0.03662628307938576\n",
      "Epoch: 1 Batch: 97 Loss: 0.10285598039627075\n",
      "Epoch: 1 Batch: 98 Loss: 0.161324605345726\n",
      "Epoch: 1 Batch: 99 Loss: 0.05211801826953888\n",
      "Epoch: 2 Batch: 0 Loss: 0.03616858273744583\n",
      "Epoch: 2 Batch: 0\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 2 Batch: 1 Loss: 0.12327396869659424\n",
      "Epoch: 2 Batch: 2 Loss: 0.04377856105566025\n",
      "Epoch: 2 Batch: 3 Loss: 0.27983957529067993\n",
      "Epoch: 2 Batch: 4 Loss: 0.14495430886745453\n",
      "Epoch: 2 Batch: 5 Loss: 0.11944787949323654\n",
      "Epoch: 2 Batch: 6 Loss: 0.22274723649024963\n",
      "Epoch: 2 Batch: 7 Loss: 0.1681867092847824\n",
      "Epoch: 2 Batch: 8 Loss: 0.19401004910469055\n",
      "Epoch: 2 Batch: 9 Loss: 0.17373806238174438\n",
      "Epoch: 2 Batch: 10 Loss: 0.16369792819023132\n",
      "Epoch: 2 Batch: 11 Loss: 0.20111167430877686\n",
      "Epoch: 2 Batch: 12 Loss: 0.10613755881786346\n",
      "Epoch: 2 Batch: 13 Loss: 0.07941445708274841\n",
      "Epoch: 2 Batch: 14 Loss: 0.06404054909944534\n",
      "Epoch: 2 Batch: 15 Loss: 0.06509777903556824\n",
      "Epoch: 2 Batch: 16 Loss: 0.26778632402420044\n",
      "Epoch: 2 Batch: 17 Loss: 0.18907566368579865\n",
      "Epoch: 2 Batch: 18 Loss: 0.1575857251882553\n",
      "Epoch: 2 Batch: 19 Loss: 0.0985007956624031\n",
      "Epoch: 2 Batch: 20 Loss: 0.09276917576789856\n",
      "Epoch: 2 Batch: 21 Loss: 0.1441819965839386\n",
      "Epoch: 2 Batch: 22 Loss: 0.12024436146020889\n",
      "Epoch: 2 Batch: 23 Loss: 0.3544006049633026\n",
      "Epoch: 2 Batch: 24 Loss: 0.06390734761953354\n",
      "Epoch: 2 Batch: 25 Loss: 0.12771821022033691\n",
      "Epoch: 2 Batch: 26 Loss: 0.05810800567269325\n",
      "Epoch: 2 Batch: 27 Loss: 0.12340602278709412\n",
      "Epoch: 2 Batch: 28 Loss: 0.06077958270907402\n",
      "Epoch: 2 Batch: 29 Loss: 0.0744657889008522\n",
      "Epoch: 2 Batch: 30 Loss: 0.11729507893323898\n",
      "Epoch: 2 Batch: 31 Loss: 0.11626124382019043\n",
      "Epoch: 2 Batch: 32 Loss: 0.08961012959480286\n",
      "Epoch: 2 Batch: 33 Loss: 0.2933976352214813\n",
      "Epoch: 2 Batch: 34 Loss: 0.039762578904628754\n",
      "Epoch: 2 Batch: 35 Loss: 0.19426561892032623\n",
      "Epoch: 2 Batch: 36 Loss: 0.04920722171664238\n",
      "Epoch: 2 Batch: 37 Loss: 0.06981144845485687\n",
      "Epoch: 2 Batch: 38 Loss: 0.05144340544939041\n",
      "Epoch: 2 Batch: 39 Loss: 0.06739316880702972\n",
      "Epoch: 2 Batch: 40 Loss: 0.10515697300434113\n",
      "Epoch: 2 Batch: 41 Loss: 0.08797105401754379\n",
      "Epoch: 2 Batch: 42 Loss: 0.22662022709846497\n",
      "Epoch: 2 Batch: 43 Loss: 0.09386177361011505\n",
      "Epoch: 2 Batch: 44 Loss: 0.08305288106203079\n",
      "Epoch: 2 Batch: 45 Loss: 0.1473608762025833\n",
      "Epoch: 2 Batch: 46 Loss: 0.0841294676065445\n",
      "Epoch: 2 Batch: 47 Loss: 0.3542425334453583\n",
      "Epoch: 2 Batch: 48 Loss: 0.16088953614234924\n",
      "Epoch: 2 Batch: 49 Loss: 0.10775856673717499\n",
      "Epoch: 2 Batch: 50 Loss: 0.08294176310300827\n",
      "Epoch: 2 Batch: 50\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 2 Batch: 51 Loss: 0.11741535365581512\n",
      "Epoch: 2 Batch: 52 Loss: 0.15240883827209473\n",
      "Epoch: 2 Batch: 53 Loss: 0.08269140869379044\n",
      "Epoch: 2 Batch: 54 Loss: 0.1016138568520546\n",
      "Epoch: 2 Batch: 55 Loss: 0.14352773129940033\n",
      "Epoch: 2 Batch: 56 Loss: 0.12485149502754211\n",
      "Epoch: 2 Batch: 57 Loss: 0.10779968649148941\n",
      "Epoch: 2 Batch: 58 Loss: 0.06788165867328644\n",
      "Epoch: 2 Batch: 59 Loss: 0.15890921652317047\n",
      "Epoch: 2 Batch: 60 Loss: 0.13901814818382263\n",
      "Epoch: 2 Batch: 61 Loss: 0.03793249651789665\n",
      "Epoch: 2 Batch: 62 Loss: 0.1593993604183197\n",
      "Epoch: 2 Batch: 63 Loss: 0.35699519515037537\n",
      "Epoch: 2 Batch: 64 Loss: 0.12008368223905563\n",
      "Epoch: 2 Batch: 65 Loss: 0.17375753819942474\n",
      "Epoch: 2 Batch: 66 Loss: 0.1451105773448944\n",
      "Epoch: 2 Batch: 67 Loss: 0.12095450609922409\n",
      "Epoch: 2 Batch: 68 Loss: 0.13398298621177673\n",
      "Epoch: 2 Batch: 69 Loss: 0.1598864495754242\n",
      "Epoch: 2 Batch: 70 Loss: 0.08463543653488159\n",
      "Epoch: 2 Batch: 71 Loss: 0.17737188935279846\n",
      "Epoch: 2 Batch: 72 Loss: 0.18061023950576782\n",
      "Epoch: 2 Batch: 73 Loss: 0.40865930914878845\n",
      "Epoch: 2 Batch: 74 Loss: 0.21111485362052917\n",
      "Epoch: 2 Batch: 75 Loss: 0.08937575668096542\n",
      "Epoch: 2 Batch: 76 Loss: 0.14388008415699005\n",
      "Epoch: 2 Batch: 77 Loss: 0.12347117811441422\n",
      "Epoch: 2 Batch: 78 Loss: 0.03327757492661476\n",
      "Epoch: 2 Batch: 79 Loss: 0.10070370137691498\n",
      "Epoch: 2 Batch: 80 Loss: 0.14358174800872803\n",
      "Epoch: 2 Batch: 81 Loss: 0.27159619331359863\n",
      "Epoch: 2 Batch: 82 Loss: 0.11776447296142578\n",
      "Epoch: 2 Batch: 83 Loss: 0.020900746807456017\n",
      "Epoch: 2 Batch: 84 Loss: 0.2605842649936676\n",
      "Epoch: 2 Batch: 85 Loss: 0.29471632838249207\n",
      "Epoch: 2 Batch: 86 Loss: 0.13919030129909515\n",
      "Epoch: 2 Batch: 87 Loss: 0.14754195511341095\n",
      "Epoch: 2 Batch: 88 Loss: 0.12435218691825867\n",
      "Epoch: 2 Batch: 89 Loss: 0.06697322428226471\n",
      "Epoch: 2 Batch: 90 Loss: 0.14395882189273834\n",
      "Epoch: 2 Batch: 91 Loss: 0.10096526890993118\n",
      "Epoch: 2 Batch: 92 Loss: 0.07928869873285294\n",
      "Epoch: 2 Batch: 93 Loss: 0.13304346799850464\n",
      "Epoch: 2 Batch: 94 Loss: 0.18569371104240417\n",
      "Epoch: 2 Batch: 95 Loss: 0.2669036388397217\n",
      "Epoch: 2 Batch: 96 Loss: 0.07342200726270676\n",
      "Epoch: 2 Batch: 97 Loss: 0.060256943106651306\n",
      "Epoch: 2 Batch: 98 Loss: 0.11515697091817856\n",
      "Epoch: 2 Batch: 99 Loss: 0.12758782505989075\n",
      "Epoch: 3 Batch: 0 Loss: 0.1000455766916275\n",
      "Epoch: 3 Batch: 0\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 3 Batch: 1 Loss: 0.06672203540802002\n",
      "Epoch: 3 Batch: 2 Loss: 0.09338375180959702\n",
      "Epoch: 3 Batch: 3 Loss: 0.12540313601493835\n",
      "Epoch: 3 Batch: 4 Loss: 0.149126335978508\n",
      "Epoch: 3 Batch: 5 Loss: 0.04445384442806244\n",
      "Epoch: 3 Batch: 6 Loss: 0.15516921877861023\n",
      "Epoch: 3 Batch: 7 Loss: 0.05899622291326523\n",
      "Epoch: 3 Batch: 8 Loss: 0.18475231528282166\n",
      "Epoch: 3 Batch: 9 Loss: 0.03494606167078018\n",
      "Epoch: 3 Batch: 10 Loss: 0.11938582360744476\n",
      "Epoch: 3 Batch: 11 Loss: 0.4017569422721863\n",
      "Epoch: 3 Batch: 12 Loss: 0.046430639922618866\n",
      "Epoch: 3 Batch: 13 Loss: 0.1476328819990158\n",
      "Epoch: 3 Batch: 14 Loss: 0.19745704531669617\n",
      "Epoch: 3 Batch: 15 Loss: 0.13533130288124084\n",
      "Epoch: 3 Batch: 16 Loss: 0.06406573206186295\n",
      "Epoch: 3 Batch: 17 Loss: 0.23191112279891968\n",
      "Epoch: 3 Batch: 18 Loss: 0.0910661593079567\n",
      "Epoch: 3 Batch: 19 Loss: 0.13571685552597046\n",
      "Epoch: 3 Batch: 20 Loss: 0.09331690520048141\n",
      "Epoch: 3 Batch: 21 Loss: 0.127145916223526\n",
      "Epoch: 3 Batch: 22 Loss: 0.2117774784564972\n",
      "Epoch: 3 Batch: 23 Loss: 0.03516451269388199\n",
      "Epoch: 3 Batch: 24 Loss: 0.14562810957431793\n",
      "Epoch: 3 Batch: 25 Loss: 0.18102288246154785\n",
      "Epoch: 3 Batch: 26 Loss: 0.07728159427642822\n",
      "Epoch: 3 Batch: 27 Loss: 0.1243593618273735\n",
      "Epoch: 3 Batch: 28 Loss: 0.10241621732711792\n",
      "Epoch: 3 Batch: 29 Loss: 0.09643567353487015\n",
      "Epoch: 3 Batch: 30 Loss: 0.056230057030916214\n",
      "Epoch: 3 Batch: 31 Loss: 0.15756723284721375\n",
      "Epoch: 3 Batch: 32 Loss: 0.13775822520256042\n",
      "Epoch: 3 Batch: 33 Loss: 0.10487103462219238\n",
      "Epoch: 3 Batch: 34 Loss: 0.15079836547374725\n",
      "Epoch: 3 Batch: 35 Loss: 0.15190261602401733\n",
      "Epoch: 3 Batch: 36 Loss: 0.26302245259284973\n",
      "Epoch: 3 Batch: 37 Loss: 0.09649787098169327\n",
      "Epoch: 3 Batch: 38 Loss: 0.13483533263206482\n",
      "Epoch: 3 Batch: 39 Loss: 0.11046838760375977\n",
      "Epoch: 3 Batch: 40 Loss: 0.030531838536262512\n",
      "Epoch: 3 Batch: 41 Loss: 0.1277916133403778\n",
      "Epoch: 3 Batch: 42 Loss: 0.14533644914627075\n",
      "Epoch: 3 Batch: 43 Loss: 0.21500231325626373\n",
      "Epoch: 3 Batch: 44 Loss: 0.11186549067497253\n",
      "Epoch: 3 Batch: 45 Loss: 0.2887619137763977\n",
      "Epoch: 3 Batch: 46 Loss: 0.10003040730953217\n",
      "Epoch: 3 Batch: 47 Loss: 0.040880702435970306\n",
      "Epoch: 3 Batch: 48 Loss: 0.1337105631828308\n",
      "Epoch: 3 Batch: 49 Loss: 0.09805052727460861\n",
      "Epoch: 3 Batch: 50 Loss: 0.09480424970388412\n",
      "Epoch: 3 Batch: 50\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 3 Batch: 51 Loss: 0.1276792287826538\n",
      "Epoch: 3 Batch: 52 Loss: 0.1278221309185028\n",
      "Epoch: 3 Batch: 53 Loss: 0.04031568020582199\n",
      "Epoch: 3 Batch: 54 Loss: 0.1241426169872284\n",
      "Epoch: 3 Batch: 55 Loss: 0.10405287891626358\n",
      "Epoch: 3 Batch: 56 Loss: 0.06508062779903412\n",
      "Epoch: 3 Batch: 57 Loss: 0.12533468008041382\n",
      "Epoch: 3 Batch: 58 Loss: 0.09969852864742279\n",
      "Epoch: 3 Batch: 59 Loss: 0.14897730946540833\n",
      "Epoch: 3 Batch: 60 Loss: 0.25228357315063477\n",
      "Epoch: 3 Batch: 61 Loss: 0.1965273916721344\n",
      "Epoch: 3 Batch: 62 Loss: 0.07079429924488068\n",
      "Epoch: 3 Batch: 63 Loss: 0.16408458352088928\n",
      "Epoch: 3 Batch: 64 Loss: 0.23265117406845093\n",
      "Epoch: 3 Batch: 65 Loss: 0.06791052967309952\n",
      "Epoch: 3 Batch: 66 Loss: 0.3467704653739929\n",
      "Epoch: 3 Batch: 67 Loss: 0.0696435496211052\n",
      "Epoch: 3 Batch: 68 Loss: 0.1494240015745163\n",
      "Epoch: 3 Batch: 69 Loss: 0.16689729690551758\n",
      "Epoch: 3 Batch: 70 Loss: 0.08394616842269897\n",
      "Epoch: 3 Batch: 71 Loss: 0.19840028882026672\n",
      "Epoch: 3 Batch: 72 Loss: 0.0353131964802742\n",
      "Epoch: 3 Batch: 73 Loss: 0.07498718053102493\n",
      "Epoch: 3 Batch: 74 Loss: 0.13209225237369537\n",
      "Epoch: 3 Batch: 75 Loss: 0.08791621774435043\n",
      "Epoch: 3 Batch: 76 Loss: 0.22408242523670197\n",
      "Epoch: 3 Batch: 77 Loss: 0.28586581349372864\n",
      "Epoch: 3 Batch: 78 Loss: 0.08693158626556396\n",
      "Epoch: 3 Batch: 79 Loss: 0.08282576501369476\n",
      "Epoch: 3 Batch: 80 Loss: 0.21958553791046143\n",
      "Epoch: 3 Batch: 81 Loss: 0.07402178645133972\n",
      "Epoch: 3 Batch: 82 Loss: 0.18452416360378265\n",
      "Epoch: 3 Batch: 83 Loss: 0.14384202659130096\n",
      "Epoch: 3 Batch: 84 Loss: 0.0727791115641594\n",
      "Epoch: 3 Batch: 85 Loss: 0.09372184425592422\n",
      "Epoch: 3 Batch: 86 Loss: 0.1547754555940628\n",
      "Epoch: 3 Batch: 87 Loss: 0.10753407329320908\n",
      "Epoch: 3 Batch: 88 Loss: 0.2072448432445526\n",
      "Epoch: 3 Batch: 89 Loss: 0.0719519779086113\n",
      "Epoch: 3 Batch: 90 Loss: 0.12701477110385895\n",
      "Epoch: 3 Batch: 91 Loss: 0.09048455953598022\n",
      "Epoch: 3 Batch: 92 Loss: 0.264243483543396\n",
      "Epoch: 3 Batch: 93 Loss: 0.07139788568019867\n",
      "Epoch: 3 Batch: 94 Loss: 0.1858631819486618\n",
      "Epoch: 3 Batch: 95 Loss: 0.17931821942329407\n",
      "Epoch: 3 Batch: 96 Loss: 0.14267423748970032\n",
      "Epoch: 3 Batch: 97 Loss: 0.11928173899650574\n",
      "Epoch: 3 Batch: 98 Loss: 0.08219204097986221\n",
      "Epoch: 3 Batch: 99 Loss: 0.0871158093214035\n",
      "Epoch: 4 Batch: 0 Loss: 0.2769218385219574\n",
      "Epoch: 4 Batch: 0\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 4 Batch: 1 Loss: 0.061857789754867554\n",
      "Epoch: 4 Batch: 2 Loss: 0.17842626571655273\n",
      "Epoch: 4 Batch: 3 Loss: 0.16549266874790192\n",
      "Epoch: 4 Batch: 4 Loss: 0.05606447905302048\n",
      "Epoch: 4 Batch: 5 Loss: 0.35409870743751526\n",
      "Epoch: 4 Batch: 6 Loss: 0.09918604791164398\n",
      "Epoch: 4 Batch: 7 Loss: 0.18631528317928314\n",
      "Epoch: 4 Batch: 8 Loss: 0.12367518991231918\n",
      "Epoch: 4 Batch: 9 Loss: 0.1072096899151802\n",
      "Epoch: 4 Batch: 10 Loss: 0.11275732517242432\n",
      "Epoch: 4 Batch: 11 Loss: 0.08097909390926361\n",
      "Epoch: 4 Batch: 12 Loss: 0.02063298225402832\n",
      "Epoch: 4 Batch: 13 Loss: 0.15459859371185303\n",
      "Epoch: 4 Batch: 14 Loss: 0.255127489566803\n",
      "Epoch: 4 Batch: 15 Loss: 0.1668984293937683\n",
      "Epoch: 4 Batch: 16 Loss: 0.0980457067489624\n",
      "Epoch: 4 Batch: 17 Loss: 0.20879359543323517\n",
      "Epoch: 4 Batch: 18 Loss: 0.08875404298305511\n",
      "Epoch: 4 Batch: 19 Loss: 0.16189217567443848\n",
      "Epoch: 4 Batch: 20 Loss: 0.2241348773241043\n",
      "Epoch: 4 Batch: 21 Loss: 0.10607502609491348\n",
      "Epoch: 4 Batch: 22 Loss: 0.1297455132007599\n",
      "Epoch: 4 Batch: 23 Loss: 0.07681471854448318\n",
      "Epoch: 4 Batch: 24 Loss: 0.13116872310638428\n",
      "Epoch: 4 Batch: 25 Loss: 0.1741163283586502\n",
      "Epoch: 4 Batch: 26 Loss: 0.14644217491149902\n",
      "Epoch: 4 Batch: 27 Loss: 0.09006215631961823\n",
      "Epoch: 4 Batch: 28 Loss: 0.1353597491979599\n",
      "Epoch: 4 Batch: 29 Loss: 0.20808905363082886\n",
      "Epoch: 4 Batch: 30 Loss: 0.08224045485258102\n",
      "Epoch: 4 Batch: 31 Loss: 0.29158103466033936\n",
      "Epoch: 4 Batch: 32 Loss: 0.3146210312843323\n",
      "Epoch: 4 Batch: 33 Loss: 0.052571918815374374\n",
      "Epoch: 4 Batch: 34 Loss: 0.14910487830638885\n",
      "Epoch: 4 Batch: 35 Loss: 0.06071775406599045\n",
      "Epoch: 4 Batch: 36 Loss: 0.028298718854784966\n",
      "Epoch: 4 Batch: 37 Loss: 0.09644484519958496\n",
      "Epoch: 4 Batch: 38 Loss: 0.13747230172157288\n",
      "Epoch: 4 Batch: 39 Loss: 0.34940844774246216\n",
      "Epoch: 4 Batch: 40 Loss: 0.09554187208414078\n",
      "Epoch: 4 Batch: 41 Loss: 0.07547368854284286\n",
      "Epoch: 4 Batch: 42 Loss: 0.16890650987625122\n",
      "Epoch: 4 Batch: 43 Loss: 0.12003414332866669\n",
      "Epoch: 4 Batch: 44 Loss: 0.0854753777384758\n",
      "Epoch: 4 Batch: 45 Loss: 0.14228375256061554\n",
      "Epoch: 4 Batch: 46 Loss: 0.15357890725135803\n",
      "Epoch: 4 Batch: 47 Loss: 0.17562048137187958\n",
      "Epoch: 4 Batch: 48 Loss: 0.051840465515851974\n",
      "Epoch: 4 Batch: 49 Loss: 0.04053312912583351\n",
      "Epoch: 4 Batch: 50 Loss: 0.09162028133869171\n",
      "Epoch: 4 Batch: 50\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 4 Batch: 51 Loss: 0.14705567061901093\n",
      "Epoch: 4 Batch: 52 Loss: 0.03320028632879257\n",
      "Epoch: 4 Batch: 53 Loss: 0.1184152364730835\n",
      "Epoch: 4 Batch: 54 Loss: 0.3242686986923218\n",
      "Epoch: 4 Batch: 55 Loss: 0.18894101679325104\n",
      "Epoch: 4 Batch: 56 Loss: 0.147386372089386\n",
      "Epoch: 4 Batch: 57 Loss: 0.18178951740264893\n",
      "Epoch: 4 Batch: 58 Loss: 0.09351884573698044\n",
      "Epoch: 4 Batch: 59 Loss: 0.12579284608364105\n",
      "Epoch: 4 Batch: 60 Loss: 0.30643928050994873\n",
      "Epoch: 4 Batch: 61 Loss: 0.09300829470157623\n",
      "Epoch: 4 Batch: 62 Loss: 0.03060689941048622\n",
      "Epoch: 4 Batch: 63 Loss: 0.2711515724658966\n",
      "Epoch: 4 Batch: 64 Loss: 0.1309821605682373\n",
      "Epoch: 4 Batch: 65 Loss: 0.1506853550672531\n",
      "Epoch: 4 Batch: 66 Loss: 0.06940910220146179\n",
      "Epoch: 4 Batch: 67 Loss: 0.03958243876695633\n",
      "Epoch: 4 Batch: 68 Loss: 0.05483785644173622\n",
      "Epoch: 4 Batch: 69 Loss: 0.04318802058696747\n",
      "Epoch: 4 Batch: 70 Loss: 0.06085631251335144\n",
      "Epoch: 4 Batch: 71 Loss: 0.1289791464805603\n",
      "Epoch: 4 Batch: 72 Loss: 0.04323766008019447\n",
      "Epoch: 4 Batch: 73 Loss: 0.05858336761593819\n",
      "Epoch: 4 Batch: 74 Loss: 0.0724930465221405\n",
      "Epoch: 4 Batch: 75 Loss: 0.035151269286870956\n",
      "Epoch: 4 Batch: 76 Loss: 0.29722702503204346\n",
      "Epoch: 4 Batch: 77 Loss: 0.09207271039485931\n",
      "Epoch: 4 Batch: 78 Loss: 0.08511019498109818\n",
      "Epoch: 4 Batch: 79 Loss: 0.10509607195854187\n",
      "Epoch: 4 Batch: 80 Loss: 0.3035096824169159\n",
      "Epoch: 4 Batch: 81 Loss: 0.12029188126325607\n",
      "Epoch: 4 Batch: 82 Loss: 0.15391968190670013\n",
      "Epoch: 4 Batch: 83 Loss: 0.1488470435142517\n",
      "Epoch: 4 Batch: 84 Loss: 0.08335772901773453\n",
      "Epoch: 4 Batch: 85 Loss: 0.08870896697044373\n",
      "Epoch: 4 Batch: 86 Loss: 0.10814187675714493\n",
      "Epoch: 4 Batch: 87 Loss: 0.09120102226734161\n",
      "Epoch: 4 Batch: 88 Loss: 0.18022622168064117\n",
      "Epoch: 4 Batch: 89 Loss: 0.09094768017530441\n",
      "Epoch: 4 Batch: 90 Loss: 0.044839609414339066\n",
      "Epoch: 4 Batch: 91 Loss: 0.11267455667257309\n",
      "Epoch: 4 Batch: 92 Loss: 0.08141489326953888\n",
      "Epoch: 4 Batch: 93 Loss: 0.148543119430542\n",
      "Epoch: 4 Batch: 94 Loss: 0.05495380610227585\n",
      "Epoch: 4 Batch: 95 Loss: 0.06660699844360352\n",
      "Epoch: 4 Batch: 96 Loss: 0.21570822596549988\n",
      "Epoch: 4 Batch: 97 Loss: 0.10625959932804108\n",
      "Epoch: 4 Batch: 98 Loss: 0.06966488808393478\n",
      "Epoch: 4 Batch: 99 Loss: 0.10089462250471115\n",
      "Epoch: 5 Batch: 0 Loss: 0.05771886557340622\n",
      "Epoch: 5 Batch: 0\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 5 Batch: 1 Loss: 0.08912352472543716\n",
      "Epoch: 5 Batch: 2 Loss: 0.3291652798652649\n",
      "Epoch: 5 Batch: 3 Loss: 0.04975111410021782\n",
      "Epoch: 5 Batch: 4 Loss: 0.0670379027724266\n",
      "Epoch: 5 Batch: 5 Loss: 0.1384412944316864\n",
      "Epoch: 5 Batch: 6 Loss: 0.14875970780849457\n",
      "Epoch: 5 Batch: 7 Loss: 0.08036550879478455\n",
      "Epoch: 5 Batch: 8 Loss: 0.28781190514564514\n",
      "Epoch: 5 Batch: 9 Loss: 0.053335051983594894\n",
      "Epoch: 5 Batch: 10 Loss: 0.04056764021515846\n",
      "Epoch: 5 Batch: 11 Loss: 0.03424648940563202\n",
      "Epoch: 5 Batch: 12 Loss: 0.15777041018009186\n",
      "Epoch: 5 Batch: 13 Loss: 0.16185572743415833\n",
      "Epoch: 5 Batch: 14 Loss: 0.13421179354190826\n",
      "Epoch: 5 Batch: 15 Loss: 0.14457501471042633\n",
      "Epoch: 5 Batch: 16 Loss: 0.1260685920715332\n",
      "Epoch: 5 Batch: 17 Loss: 0.13539449870586395\n",
      "Epoch: 5 Batch: 18 Loss: 0.08883101493120193\n",
      "Epoch: 5 Batch: 19 Loss: 0.09953556209802628\n",
      "Epoch: 5 Batch: 20 Loss: 0.18783704936504364\n",
      "Epoch: 5 Batch: 21 Loss: 0.1498470902442932\n",
      "Epoch: 5 Batch: 22 Loss: 0.06951547414064407\n",
      "Epoch: 5 Batch: 23 Loss: 0.06596693396568298\n",
      "Epoch: 5 Batch: 24 Loss: 0.042334482073783875\n",
      "Epoch: 5 Batch: 25 Loss: 0.13226768374443054\n",
      "Epoch: 5 Batch: 26 Loss: 0.09402191638946533\n",
      "Epoch: 5 Batch: 27 Loss: 0.13471761345863342\n",
      "Epoch: 5 Batch: 28 Loss: 0.4190129041671753\n",
      "Epoch: 5 Batch: 29 Loss: 0.14305151998996735\n",
      "Epoch: 5 Batch: 30 Loss: 0.16137714684009552\n",
      "Epoch: 5 Batch: 31 Loss: 0.10711128264665604\n",
      "Epoch: 5 Batch: 32 Loss: 0.07051245123147964\n",
      "Epoch: 5 Batch: 33 Loss: 0.057058967649936676\n",
      "Epoch: 5 Batch: 34 Loss: 0.16740640997886658\n",
      "Epoch: 5 Batch: 35 Loss: 0.17425169050693512\n",
      "Epoch: 5 Batch: 36 Loss: 0.1040760800242424\n",
      "Epoch: 5 Batch: 37 Loss: 0.05062372609972954\n",
      "Epoch: 5 Batch: 38 Loss: 0.07594958692789078\n",
      "Epoch: 5 Batch: 39 Loss: 0.11770714074373245\n",
      "Epoch: 5 Batch: 40 Loss: 0.12934942543506622\n",
      "Epoch: 5 Batch: 41 Loss: 0.08818629384040833\n",
      "Epoch: 5 Batch: 42 Loss: 0.046613406389951706\n",
      "Epoch: 5 Batch: 43 Loss: 0.3669030964374542\n",
      "Epoch: 5 Batch: 44 Loss: 0.10527205467224121\n",
      "Epoch: 5 Batch: 45 Loss: 0.1260160207748413\n",
      "Epoch: 5 Batch: 46 Loss: 0.13565434515476227\n",
      "Epoch: 5 Batch: 47 Loss: 0.06969311833381653\n",
      "Epoch: 5 Batch: 48 Loss: 0.09355450421571732\n",
      "Epoch: 5 Batch: 49 Loss: 0.2044650763273239\n",
      "Epoch: 5 Batch: 50 Loss: 0.1586286574602127\n",
      "Epoch: 5 Batch: 50\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 5 Batch: 51 Loss: 0.16732065379619598\n",
      "Epoch: 5 Batch: 52 Loss: 0.09722521156072617\n",
      "Epoch: 5 Batch: 53 Loss: 0.07515392452478409\n",
      "Epoch: 5 Batch: 54 Loss: 0.0734601765871048\n",
      "Epoch: 5 Batch: 55 Loss: 0.046407025307416916\n",
      "Epoch: 5 Batch: 56 Loss: 0.10979928076267242\n",
      "Epoch: 5 Batch: 57 Loss: 0.36133456230163574\n",
      "Epoch: 5 Batch: 58 Loss: 0.11052779853343964\n",
      "Epoch: 5 Batch: 59 Loss: 0.10825130343437195\n",
      "Epoch: 5 Batch: 60 Loss: 0.22979094088077545\n",
      "Epoch: 5 Batch: 61 Loss: 0.13737301528453827\n",
      "Epoch: 5 Batch: 62 Loss: 0.12388499081134796\n",
      "Epoch: 5 Batch: 63 Loss: 0.023792410269379616\n",
      "Epoch: 5 Batch: 64 Loss: 0.12174874544143677\n",
      "Epoch: 5 Batch: 65 Loss: 0.15588733553886414\n",
      "Epoch: 5 Batch: 66 Loss: 0.12434116750955582\n",
      "Epoch: 5 Batch: 67 Loss: 0.08100602775812149\n",
      "Epoch: 5 Batch: 68 Loss: 0.15047329664230347\n",
      "Epoch: 5 Batch: 69 Loss: 0.05619990453124046\n",
      "Epoch: 5 Batch: 70 Loss: 0.07094506919384003\n",
      "Epoch: 5 Batch: 71 Loss: 0.13817203044891357\n",
      "Epoch: 5 Batch: 72 Loss: 0.43497759103775024\n",
      "Epoch: 5 Batch: 73 Loss: 0.08489128947257996\n",
      "Epoch: 5 Batch: 74 Loss: 0.029685713350772858\n",
      "Epoch: 5 Batch: 75 Loss: 0.18596044182777405\n",
      "Epoch: 5 Batch: 76 Loss: 0.18425650894641876\n",
      "Epoch: 5 Batch: 77 Loss: 0.18483629822731018\n",
      "Epoch: 5 Batch: 78 Loss: 0.156373992562294\n",
      "Epoch: 5 Batch: 79 Loss: 0.23859208822250366\n",
      "Epoch: 5 Batch: 80 Loss: 0.33367615938186646\n",
      "Epoch: 5 Batch: 81 Loss: 0.16122961044311523\n",
      "Epoch: 5 Batch: 82 Loss: 0.14931994676589966\n",
      "Epoch: 5 Batch: 83 Loss: 0.15297846496105194\n",
      "Epoch: 5 Batch: 84 Loss: 0.02229457162320614\n",
      "Epoch: 5 Batch: 85 Loss: 0.05251095071434975\n",
      "Epoch: 5 Batch: 86 Loss: 0.28675708174705505\n",
      "Epoch: 5 Batch: 87 Loss: 0.04413825273513794\n",
      "Epoch: 5 Batch: 88 Loss: 0.07230497896671295\n",
      "Epoch: 5 Batch: 89 Loss: 0.051425449550151825\n",
      "Epoch: 5 Batch: 90 Loss: 0.11814072728157043\n",
      "Epoch: 5 Batch: 91 Loss: 0.12123668193817139\n",
      "Epoch: 5 Batch: 92 Loss: 0.13477307558059692\n",
      "Epoch: 5 Batch: 93 Loss: 0.04942319542169571\n",
      "Epoch: 5 Batch: 94 Loss: 0.16896581649780273\n",
      "Epoch: 5 Batch: 95 Loss: 0.09905841201543808\n",
      "Epoch: 5 Batch: 96 Loss: 0.03591611981391907\n",
      "Epoch: 5 Batch: 97 Loss: 0.06650715321302414\n",
      "Epoch: 5 Batch: 98 Loss: 0.1134946197271347\n",
      "Epoch: 5 Batch: 99 Loss: 0.10354328155517578\n",
      "Epoch: 6 Batch: 0 Loss: 0.01934661529958248\n",
      "Epoch: 6 Batch: 0\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 6 Batch: 1 Loss: 0.11209867894649506\n",
      "Epoch: 6 Batch: 2 Loss: 0.08427946269512177\n",
      "Epoch: 6 Batch: 3 Loss: 0.07203961908817291\n",
      "Epoch: 6 Batch: 4 Loss: 0.11571759730577469\n",
      "Epoch: 6 Batch: 5 Loss: 0.08319962024688721\n",
      "Epoch: 6 Batch: 6 Loss: 0.17800264060497284\n",
      "Epoch: 6 Batch: 7 Loss: 0.31518658995628357\n",
      "Epoch: 6 Batch: 8 Loss: 0.07368176430463791\n",
      "Epoch: 6 Batch: 9 Loss: 0.13905051350593567\n",
      "Epoch: 6 Batch: 10 Loss: 0.10376585274934769\n",
      "Epoch: 6 Batch: 11 Loss: 0.061406318098306656\n",
      "Epoch: 6 Batch: 12 Loss: 0.14447781443595886\n",
      "Epoch: 6 Batch: 13 Loss: 0.10719359666109085\n",
      "Epoch: 6 Batch: 14 Loss: 0.08023668080568314\n",
      "Epoch: 6 Batch: 15 Loss: 0.18304428458213806\n",
      "Epoch: 6 Batch: 16 Loss: 0.10952650755643845\n",
      "Epoch: 6 Batch: 17 Loss: 0.14086443185806274\n",
      "Epoch: 6 Batch: 18 Loss: 0.14355795085430145\n",
      "Epoch: 6 Batch: 19 Loss: 0.06826402992010117\n",
      "Epoch: 6 Batch: 20 Loss: 0.06825555860996246\n",
      "Epoch: 6 Batch: 21 Loss: 0.08423691987991333\n",
      "Epoch: 6 Batch: 22 Loss: 0.10556916892528534\n",
      "Epoch: 6 Batch: 23 Loss: 0.13832587003707886\n",
      "Epoch: 6 Batch: 24 Loss: 0.03728658705949783\n",
      "Epoch: 6 Batch: 25 Loss: 0.08741767704486847\n",
      "Epoch: 6 Batch: 26 Loss: 0.04353044927120209\n",
      "Epoch: 6 Batch: 27 Loss: 0.04200940206646919\n",
      "Epoch: 6 Batch: 28 Loss: 0.17008350789546967\n",
      "Epoch: 6 Batch: 29 Loss: 0.30882951617240906\n",
      "Epoch: 6 Batch: 30 Loss: 0.05046558007597923\n",
      "Epoch: 6 Batch: 31 Loss: 0.15819963812828064\n",
      "Epoch: 6 Batch: 32 Loss: 0.09195055812597275\n",
      "Epoch: 6 Batch: 33 Loss: 0.03327106684446335\n",
      "Epoch: 6 Batch: 34 Loss: 0.170924112200737\n",
      "Epoch: 6 Batch: 35 Loss: 0.08556503057479858\n",
      "Epoch: 6 Batch: 36 Loss: 0.08688078075647354\n",
      "Epoch: 6 Batch: 37 Loss: 0.1480761468410492\n",
      "Epoch: 6 Batch: 38 Loss: 0.10723257064819336\n",
      "Epoch: 6 Batch: 39 Loss: 0.0875072181224823\n",
      "Epoch: 6 Batch: 40 Loss: 0.42121344804763794\n",
      "Epoch: 6 Batch: 41 Loss: 0.1178864911198616\n",
      "Epoch: 6 Batch: 42 Loss: 0.21742486953735352\n",
      "Epoch: 6 Batch: 43 Loss: 0.0854680985212326\n",
      "Epoch: 6 Batch: 44 Loss: 0.2260502576828003\n",
      "Epoch: 6 Batch: 45 Loss: 0.06758543848991394\n",
      "Epoch: 6 Batch: 46 Loss: 0.1850491315126419\n",
      "Epoch: 6 Batch: 47 Loss: 0.06342902779579163\n",
      "Epoch: 6 Batch: 48 Loss: 0.122213214635849\n",
      "Epoch: 6 Batch: 49 Loss: 0.0808025673031807\n",
      "Epoch: 6 Batch: 50 Loss: 0.1155448779463768\n",
      "Epoch: 6 Batch: 50\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 6 Batch: 51 Loss: 0.16928978264331818\n",
      "Epoch: 6 Batch: 52 Loss: 0.09737178683280945\n",
      "Epoch: 6 Batch: 53 Loss: 0.061635129153728485\n",
      "Epoch: 6 Batch: 54 Loss: 0.09853501617908478\n",
      "Epoch: 6 Batch: 55 Loss: 0.12129386514425278\n",
      "Epoch: 6 Batch: 56 Loss: 0.14161477982997894\n",
      "Epoch: 6 Batch: 57 Loss: 0.04605334997177124\n",
      "Epoch: 6 Batch: 58 Loss: 0.07939551025629044\n",
      "Epoch: 6 Batch: 59 Loss: 0.19820402562618256\n",
      "Epoch: 6 Batch: 60 Loss: 0.06294956803321838\n",
      "Epoch: 6 Batch: 61 Loss: 0.14545343816280365\n",
      "Epoch: 6 Batch: 62 Loss: 0.15664538741111755\n",
      "Epoch: 6 Batch: 63 Loss: 0.07320529967546463\n",
      "Epoch: 6 Batch: 64 Loss: 0.10358336567878723\n",
      "Epoch: 6 Batch: 65 Loss: 0.10954228043556213\n",
      "Epoch: 6 Batch: 66 Loss: 0.14329597353935242\n",
      "Epoch: 6 Batch: 67 Loss: 0.01262580044567585\n",
      "Epoch: 6 Batch: 68 Loss: 0.06925444304943085\n",
      "Epoch: 6 Batch: 69 Loss: 0.07579690963029861\n",
      "Epoch: 6 Batch: 70 Loss: 0.1918889284133911\n",
      "Epoch: 6 Batch: 71 Loss: 0.0735725462436676\n",
      "Epoch: 6 Batch: 72 Loss: 0.04200507700443268\n",
      "Epoch: 6 Batch: 73 Loss: 0.03349993750452995\n",
      "Epoch: 6 Batch: 74 Loss: 0.27762797474861145\n",
      "Epoch: 6 Batch: 75 Loss: 0.2306707799434662\n",
      "Epoch: 6 Batch: 76 Loss: 0.24957206845283508\n",
      "Epoch: 6 Batch: 77 Loss: 0.07536487281322479\n",
      "Epoch: 6 Batch: 78 Loss: 0.1396297812461853\n",
      "Epoch: 6 Batch: 79 Loss: 0.11361492425203323\n",
      "Epoch: 6 Batch: 80 Loss: 0.23101654648780823\n",
      "Epoch: 6 Batch: 81 Loss: 0.14293284714221954\n",
      "Epoch: 6 Batch: 82 Loss: 0.08836041390895844\n",
      "Epoch: 6 Batch: 83 Loss: 0.1231982558965683\n",
      "Epoch: 6 Batch: 84 Loss: 0.05074894800782204\n",
      "Epoch: 6 Batch: 85 Loss: 0.11985711008310318\n",
      "Epoch: 6 Batch: 86 Loss: 0.13813552260398865\n",
      "Epoch: 6 Batch: 87 Loss: 0.0983574315905571\n",
      "Epoch: 6 Batch: 88 Loss: 0.12743327021598816\n",
      "Epoch: 6 Batch: 89 Loss: 0.0264405719935894\n",
      "Epoch: 6 Batch: 90 Loss: 0.4883945882320404\n",
      "Epoch: 6 Batch: 91 Loss: 0.16847729682922363\n",
      "Epoch: 6 Batch: 92 Loss: 0.11138305068016052\n",
      "Epoch: 6 Batch: 93 Loss: 0.10015131533145905\n",
      "Epoch: 6 Batch: 94 Loss: 0.028588570654392242\n",
      "Epoch: 6 Batch: 95 Loss: 0.24338391423225403\n",
      "Epoch: 6 Batch: 96 Loss: 0.13015735149383545\n",
      "Epoch: 6 Batch: 97 Loss: 0.12945464253425598\n",
      "Epoch: 6 Batch: 98 Loss: 0.3742462694644928\n",
      "Epoch: 6 Batch: 99 Loss: 0.1371244490146637\n",
      "Epoch: 7 Batch: 0 Loss: 0.12915284931659698\n",
      "Epoch: 7 Batch: 0\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 7 Batch: 1 Loss: 0.13929812610149384\n",
      "Epoch: 7 Batch: 2 Loss: 0.05836813896894455\n",
      "Epoch: 7 Batch: 3 Loss: 0.0933670848608017\n",
      "Epoch: 7 Batch: 4 Loss: 0.0275342408567667\n",
      "Epoch: 7 Batch: 5 Loss: 0.09106491506099701\n",
      "Epoch: 7 Batch: 6 Loss: 0.138162299990654\n",
      "Epoch: 7 Batch: 7 Loss: 0.2060135006904602\n",
      "Epoch: 7 Batch: 8 Loss: 0.1783207803964615\n",
      "Epoch: 7 Batch: 9 Loss: 0.039906494319438934\n",
      "Epoch: 7 Batch: 10 Loss: 0.20820149779319763\n",
      "Epoch: 7 Batch: 11 Loss: 0.09212659299373627\n",
      "Epoch: 7 Batch: 12 Loss: 0.06947673857212067\n",
      "Epoch: 7 Batch: 13 Loss: 0.15794995427131653\n",
      "Epoch: 7 Batch: 14 Loss: 0.07790865749120712\n",
      "Epoch: 7 Batch: 15 Loss: 0.12468963861465454\n",
      "Epoch: 7 Batch: 16 Loss: 0.1540575921535492\n",
      "Epoch: 7 Batch: 17 Loss: 0.10011579096317291\n",
      "Epoch: 7 Batch: 18 Loss: 0.2171737253665924\n",
      "Epoch: 7 Batch: 19 Loss: 0.16384388506412506\n",
      "Epoch: 7 Batch: 20 Loss: 0.11036354303359985\n",
      "Epoch: 7 Batch: 21 Loss: 0.02295355312526226\n",
      "Epoch: 7 Batch: 22 Loss: 0.4003969430923462\n",
      "Epoch: 7 Batch: 23 Loss: 0.1836216151714325\n",
      "Epoch: 7 Batch: 24 Loss: 0.08096473664045334\n",
      "Epoch: 7 Batch: 25 Loss: 0.12708322703838348\n",
      "Epoch: 7 Batch: 26 Loss: 0.1659666746854782\n",
      "Epoch: 7 Batch: 27 Loss: 0.08918260782957077\n",
      "Epoch: 7 Batch: 28 Loss: 0.11842328310012817\n",
      "Epoch: 7 Batch: 29 Loss: 0.09447705745697021\n",
      "Epoch: 7 Batch: 30 Loss: 0.12933997809886932\n",
      "Epoch: 7 Batch: 31 Loss: 0.1395723670721054\n",
      "Epoch: 7 Batch: 32 Loss: 0.10274184495210648\n",
      "Epoch: 7 Batch: 33 Loss: 0.048214416950941086\n",
      "Epoch: 7 Batch: 34 Loss: 0.1327497363090515\n",
      "Epoch: 7 Batch: 35 Loss: 0.22461555898189545\n",
      "Epoch: 7 Batch: 36 Loss: 0.08255685120820999\n",
      "Epoch: 7 Batch: 37 Loss: 0.09964781999588013\n",
      "Epoch: 7 Batch: 38 Loss: 0.14592450857162476\n",
      "Epoch: 7 Batch: 39 Loss: 0.3072412610054016\n",
      "Epoch: 7 Batch: 40 Loss: 0.12105704843997955\n",
      "Epoch: 7 Batch: 41 Loss: 0.05215775966644287\n",
      "Epoch: 7 Batch: 42 Loss: 0.0790434405207634\n",
      "Epoch: 7 Batch: 43 Loss: 0.04552248492836952\n",
      "Epoch: 7 Batch: 44 Loss: 0.10531176626682281\n",
      "Epoch: 7 Batch: 45 Loss: 0.03818801790475845\n",
      "Epoch: 7 Batch: 46 Loss: 0.003986472729593515\n",
      "Epoch: 7 Batch: 47 Loss: 0.09215925633907318\n",
      "Epoch: 7 Batch: 48 Loss: 0.2241780310869217\n",
      "Epoch: 7 Batch: 49 Loss: 0.4595670700073242\n",
      "Epoch: 7 Batch: 50 Loss: 0.05598603934049606\n",
      "Epoch: 7 Batch: 50\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 7 Batch: 51 Loss: 0.09570136666297913\n",
      "Epoch: 7 Batch: 52 Loss: 0.3750307857990265\n",
      "Epoch: 7 Batch: 53 Loss: 0.09521268308162689\n",
      "Epoch: 7 Batch: 54 Loss: 0.13935653865337372\n",
      "Epoch: 7 Batch: 55 Loss: 0.08181940019130707\n",
      "Epoch: 7 Batch: 56 Loss: 0.08510036766529083\n",
      "Epoch: 7 Batch: 57 Loss: 0.05794120952486992\n",
      "Epoch: 7 Batch: 58 Loss: 0.18562310934066772\n",
      "Epoch: 7 Batch: 59 Loss: 0.11056581139564514\n",
      "Epoch: 7 Batch: 60 Loss: 0.2231670618057251\n",
      "Epoch: 7 Batch: 61 Loss: 0.08715575933456421\n",
      "Epoch: 7 Batch: 62 Loss: 0.11603591591119766\n",
      "Epoch: 7 Batch: 63 Loss: 0.23484598100185394\n",
      "Epoch: 7 Batch: 64 Loss: 0.13361182808876038\n",
      "Epoch: 7 Batch: 65 Loss: 0.09813595563173294\n",
      "Epoch: 7 Batch: 66 Loss: 0.11198265105485916\n",
      "Epoch: 7 Batch: 67 Loss: 0.034815460443496704\n",
      "Epoch: 7 Batch: 68 Loss: 0.15520739555358887\n",
      "Epoch: 7 Batch: 69 Loss: 0.2871265113353729\n",
      "Epoch: 7 Batch: 70 Loss: 0.04646120220422745\n",
      "Epoch: 7 Batch: 71 Loss: 0.12142432481050491\n",
      "Epoch: 7 Batch: 72 Loss: 0.10471764206886292\n",
      "Epoch: 7 Batch: 73 Loss: 0.05596425011754036\n",
      "Epoch: 7 Batch: 74 Loss: 0.06709069758653641\n",
      "Epoch: 7 Batch: 75 Loss: 0.09573284536600113\n",
      "Epoch: 7 Batch: 76 Loss: 0.05563822016119957\n",
      "Epoch: 7 Batch: 77 Loss: 0.10692163556814194\n",
      "Epoch: 7 Batch: 78 Loss: 0.05405494198203087\n",
      "Epoch: 7 Batch: 79 Loss: 0.06654605269432068\n",
      "Epoch: 7 Batch: 80 Loss: 0.11795197427272797\n",
      "Epoch: 7 Batch: 81 Loss: 0.08254396915435791\n",
      "Epoch: 7 Batch: 82 Loss: 0.05059319734573364\n",
      "Epoch: 7 Batch: 83 Loss: 0.24809220433235168\n",
      "Epoch: 7 Batch: 84 Loss: 0.20342537760734558\n",
      "Epoch: 7 Batch: 85 Loss: 0.14343084394931793\n",
      "Epoch: 7 Batch: 86 Loss: 0.23804199695587158\n",
      "Epoch: 7 Batch: 87 Loss: 0.11474675685167313\n",
      "Epoch: 7 Batch: 88 Loss: 0.1179409995675087\n",
      "Epoch: 7 Batch: 89 Loss: 0.23471027612686157\n",
      "Epoch: 7 Batch: 90 Loss: 0.0798528864979744\n",
      "Epoch: 7 Batch: 91 Loss: 0.062313180416822433\n",
      "Epoch: 7 Batch: 92 Loss: 0.21776831150054932\n",
      "Epoch: 7 Batch: 93 Loss: 0.06584572046995163\n",
      "Epoch: 7 Batch: 94 Loss: 0.0337790846824646\n",
      "Epoch: 7 Batch: 95 Loss: 0.09418794512748718\n",
      "Epoch: 7 Batch: 96 Loss: 0.11427851021289825\n",
      "Epoch: 7 Batch: 97 Loss: 0.3977580964565277\n",
      "Epoch: 7 Batch: 98 Loss: 0.040462054312229156\n",
      "Epoch: 7 Batch: 99 Loss: 0.11625200510025024\n",
      "Epoch: 8 Batch: 0 Loss: 0.2930304706096649\n",
      "Epoch: 8 Batch: 0\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 8 Batch: 1 Loss: 0.12269960343837738\n",
      "Epoch: 8 Batch: 2 Loss: 0.1844744086265564\n",
      "Epoch: 8 Batch: 3 Loss: 0.13930293917655945\n",
      "Epoch: 8 Batch: 4 Loss: 0.11188431084156036\n",
      "Epoch: 8 Batch: 5 Loss: 0.09310353547334671\n",
      "Epoch: 8 Batch: 6 Loss: 0.07645541429519653\n",
      "Epoch: 8 Batch: 7 Loss: 0.12124975770711899\n",
      "Epoch: 8 Batch: 8 Loss: 0.12088178843259811\n",
      "Epoch: 8 Batch: 9 Loss: 0.07168743759393692\n",
      "Epoch: 8 Batch: 10 Loss: 0.14346523582935333\n",
      "Epoch: 8 Batch: 11 Loss: 0.08639797568321228\n",
      "Epoch: 8 Batch: 12 Loss: 0.025253083556890488\n",
      "Epoch: 8 Batch: 13 Loss: 0.0944671481847763\n",
      "Epoch: 8 Batch: 14 Loss: 0.09010927379131317\n",
      "Epoch: 8 Batch: 15 Loss: 0.1264672577381134\n",
      "Epoch: 8 Batch: 16 Loss: 0.1313154399394989\n",
      "Epoch: 8 Batch: 17 Loss: 0.026537613943219185\n",
      "Epoch: 8 Batch: 18 Loss: 0.07121247053146362\n",
      "Epoch: 8 Batch: 19 Loss: 0.06770524382591248\n",
      "Epoch: 8 Batch: 20 Loss: 0.0985490083694458\n",
      "Epoch: 8 Batch: 21 Loss: 0.11147412657737732\n",
      "Epoch: 8 Batch: 22 Loss: 0.21091891825199127\n",
      "Epoch: 8 Batch: 23 Loss: 0.08768878132104874\n",
      "Epoch: 8 Batch: 24 Loss: 0.2173386812210083\n",
      "Epoch: 8 Batch: 25 Loss: 0.3737730383872986\n",
      "Epoch: 8 Batch: 26 Loss: 0.10812150686979294\n",
      "Epoch: 8 Batch: 27 Loss: 0.025083251297473907\n",
      "Epoch: 8 Batch: 28 Loss: 0.07581257820129395\n",
      "Epoch: 8 Batch: 29 Loss: 0.13978826999664307\n",
      "Epoch: 8 Batch: 30 Loss: 0.03278619050979614\n",
      "Epoch: 8 Batch: 31 Loss: 0.16978593170642853\n",
      "Epoch: 8 Batch: 32 Loss: 0.09536683559417725\n",
      "Epoch: 8 Batch: 33 Loss: 0.12664136290550232\n",
      "Epoch: 8 Batch: 34 Loss: 0.09412579238414764\n",
      "Epoch: 8 Batch: 35 Loss: 0.1874660849571228\n",
      "Epoch: 8 Batch: 36 Loss: 0.11226727068424225\n",
      "Epoch: 8 Batch: 37 Loss: 0.09951139241456985\n",
      "Epoch: 8 Batch: 38 Loss: 0.13511231541633606\n",
      "Epoch: 8 Batch: 39 Loss: 0.10593706369400024\n",
      "Epoch: 8 Batch: 40 Loss: 0.1493409276008606\n",
      "Epoch: 8 Batch: 41 Loss: 0.19305776059627533\n",
      "Epoch: 8 Batch: 42 Loss: 0.061235908418893814\n",
      "Epoch: 8 Batch: 43 Loss: 0.02336708828806877\n",
      "Epoch: 8 Batch: 44 Loss: 0.17524641752243042\n",
      "Epoch: 8 Batch: 45 Loss: 0.1257735639810562\n",
      "Epoch: 8 Batch: 46 Loss: 0.1977313756942749\n",
      "Epoch: 8 Batch: 47 Loss: 0.20576795935630798\n",
      "Epoch: 8 Batch: 48 Loss: 0.0722736194729805\n",
      "Epoch: 8 Batch: 49 Loss: 0.1882244050502777\n",
      "Epoch: 8 Batch: 50 Loss: 0.17077448964118958\n",
      "Epoch: 8 Batch: 50\n",
      "Train acc: 0.9378125\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 8 Batch: 51 Loss: 0.009956001304090023\n",
      "Epoch: 8 Batch: 52 Loss: 0.04764311760663986\n",
      "Epoch: 8 Batch: 53 Loss: 0.049395602196455\n",
      "Epoch: 8 Batch: 54 Loss: 0.05667858570814133\n",
      "Epoch: 8 Batch: 55 Loss: 0.04529833793640137\n",
      "Epoch: 8 Batch: 56 Loss: 0.21347777545452118\n",
      "Epoch: 8 Batch: 57 Loss: 0.04684038087725639\n",
      "Epoch: 8 Batch: 58 Loss: 0.11793236434459686\n",
      "Epoch: 8 Batch: 59 Loss: 0.09905171394348145\n",
      "Epoch: 8 Batch: 60 Loss: 0.12212644517421722\n",
      "Epoch: 8 Batch: 61 Loss: 0.04196815565228462\n",
      "Epoch: 8 Batch: 62 Loss: 0.14376375079154968\n",
      "Epoch: 8 Batch: 63 Loss: 0.4118242859840393\n",
      "Epoch: 8 Batch: 64 Loss: 0.03694290295243263\n",
      "Epoch: 8 Batch: 65 Loss: 0.16366156935691833\n",
      "Epoch: 8 Batch: 66 Loss: 0.08114346861839294\n",
      "Epoch: 8 Batch: 67 Loss: 0.21699370443820953\n",
      "Epoch: 8 Batch: 68 Loss: 0.1103430837392807\n",
      "Epoch: 8 Batch: 69 Loss: 0.14051169157028198\n",
      "Epoch: 8 Batch: 70 Loss: 0.12787480652332306\n",
      "Epoch: 8 Batch: 71 Loss: 0.09105907380580902\n",
      "Epoch: 8 Batch: 72 Loss: 0.10340113937854767\n",
      "Epoch: 8 Batch: 73 Loss: 0.07429302483797073\n",
      "Epoch: 8 Batch: 74 Loss: 0.0379774384200573\n",
      "Epoch: 8 Batch: 75 Loss: 0.025233885273337364\n",
      "Epoch: 8 Batch: 76 Loss: 0.11982832849025726\n",
      "Epoch: 8 Batch: 77 Loss: 0.22903650999069214\n",
      "Epoch: 8 Batch: 78 Loss: 0.0905148983001709\n",
      "Epoch: 8 Batch: 79 Loss: 0.2715688645839691\n",
      "Epoch: 8 Batch: 80 Loss: 0.08043595403432846\n",
      "Epoch: 8 Batch: 81 Loss: 0.4411388039588928\n",
      "Epoch: 8 Batch: 82 Loss: 0.43791747093200684\n",
      "Epoch: 8 Batch: 83 Loss: 0.12986081838607788\n",
      "Epoch: 8 Batch: 84 Loss: 0.10194619745016098\n",
      "Epoch: 8 Batch: 85 Loss: 0.03275713697075844\n",
      "Epoch: 8 Batch: 86 Loss: 0.15401829779148102\n",
      "Epoch: 8 Batch: 87 Loss: 0.1292385756969452\n",
      "Epoch: 8 Batch: 88 Loss: 0.05542587488889694\n",
      "Epoch: 8 Batch: 89 Loss: 0.15252314507961273\n",
      "Epoch: 8 Batch: 90 Loss: 0.2014860212802887\n",
      "Epoch: 8 Batch: 91 Loss: 0.1072206199169159\n",
      "Epoch: 8 Batch: 92 Loss: 0.13717299699783325\n",
      "Epoch: 8 Batch: 93 Loss: 0.09137510508298874\n",
      "Epoch: 8 Batch: 94 Loss: 0.06765604019165039\n",
      "Epoch: 8 Batch: 95 Loss: 0.1569620966911316\n",
      "Epoch: 8 Batch: 96 Loss: 0.1991681456565857\n",
      "Epoch: 8 Batch: 97 Loss: 0.09496248513460159\n",
      "Epoch: 8 Batch: 98 Loss: 0.06767892837524414\n",
      "Epoch: 8 Batch: 99 Loss: 0.07987793534994125\n",
      "Epoch: 9 Batch: 0 Loss: 0.05581151321530342\n",
      "Epoch: 9 Batch: 0\n",
      "Train acc: 0.938125\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 9 Batch: 1 Loss: 0.41752034425735474\n",
      "Epoch: 9 Batch: 2 Loss: 0.028876157477498055\n",
      "Epoch: 9 Batch: 3 Loss: 0.21686270833015442\n",
      "Epoch: 9 Batch: 4 Loss: 0.08423329889774323\n",
      "Epoch: 9 Batch: 5 Loss: 0.26574012637138367\n",
      "Epoch: 9 Batch: 6 Loss: 0.13018237054347992\n",
      "Epoch: 9 Batch: 7 Loss: 0.19074347615242004\n",
      "Epoch: 9 Batch: 8 Loss: 0.25809621810913086\n",
      "Epoch: 9 Batch: 9 Loss: 0.06614284217357635\n",
      "Epoch: 9 Batch: 10 Loss: 0.07570204883813858\n",
      "Epoch: 9 Batch: 11 Loss: 0.3412284553050995\n",
      "Epoch: 9 Batch: 12 Loss: 0.19050860404968262\n",
      "Epoch: 9 Batch: 13 Loss: 0.02824498899281025\n",
      "Epoch: 9 Batch: 14 Loss: 0.09133695811033249\n",
      "Epoch: 9 Batch: 15 Loss: 0.0518731065094471\n",
      "Epoch: 9 Batch: 16 Loss: 0.037215325981378555\n",
      "Epoch: 9 Batch: 17 Loss: 0.18516094982624054\n",
      "Epoch: 9 Batch: 18 Loss: 0.04024709388613701\n",
      "Epoch: 9 Batch: 19 Loss: 0.1362016499042511\n",
      "Epoch: 9 Batch: 20 Loss: 0.03649255633354187\n",
      "Epoch: 9 Batch: 21 Loss: 0.04579636827111244\n",
      "Epoch: 9 Batch: 22 Loss: 0.09148449450731277\n",
      "Epoch: 9 Batch: 23 Loss: 0.13876836001873016\n",
      "Epoch: 9 Batch: 24 Loss: 0.09098145365715027\n",
      "Epoch: 9 Batch: 25 Loss: 0.09845271706581116\n",
      "Epoch: 9 Batch: 26 Loss: 0.018759464845061302\n",
      "Epoch: 9 Batch: 27 Loss: 0.09222407639026642\n",
      "Epoch: 9 Batch: 28 Loss: 0.10172484815120697\n",
      "Epoch: 9 Batch: 29 Loss: 0.13414998352527618\n",
      "Epoch: 9 Batch: 30 Loss: 0.03642772510647774\n",
      "Epoch: 9 Batch: 31 Loss: 0.06904913485050201\n",
      "Epoch: 9 Batch: 32 Loss: 0.08537165820598602\n",
      "Epoch: 9 Batch: 33 Loss: 0.06979013979434967\n",
      "Epoch: 9 Batch: 34 Loss: 0.08247876167297363\n",
      "Epoch: 9 Batch: 35 Loss: 0.19795218110084534\n",
      "Epoch: 9 Batch: 36 Loss: 0.10610831528902054\n",
      "Epoch: 9 Batch: 37 Loss: 0.04576529562473297\n",
      "Epoch: 9 Batch: 38 Loss: 0.24565961956977844\n",
      "Epoch: 9 Batch: 39 Loss: 0.11206035315990448\n",
      "Epoch: 9 Batch: 40 Loss: 0.043493397533893585\n",
      "Epoch: 9 Batch: 41 Loss: 0.1363455206155777\n",
      "Epoch: 9 Batch: 42 Loss: 0.04946700483560562\n",
      "Epoch: 9 Batch: 43 Loss: 0.1106119453907013\n",
      "Epoch: 9 Batch: 44 Loss: 0.07780960947275162\n",
      "Epoch: 9 Batch: 45 Loss: 0.10408651828765869\n",
      "Epoch: 9 Batch: 46 Loss: 0.22934970259666443\n",
      "Epoch: 9 Batch: 47 Loss: 0.31927379965782166\n",
      "Epoch: 9 Batch: 48 Loss: 0.09772209823131561\n",
      "Epoch: 9 Batch: 49 Loss: 0.06420336663722992\n",
      "Epoch: 9 Batch: 50 Loss: 0.08995822072029114\n",
      "Epoch: 9 Batch: 50\n",
      "Train acc: 0.9375\n",
      "Test acc: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 9 Batch: 51 Loss: 0.030829349532723427\n",
      "Epoch: 9 Batch: 52 Loss: 0.1444762945175171\n",
      "Epoch: 9 Batch: 53 Loss: 0.11651355028152466\n",
      "Epoch: 9 Batch: 54 Loss: 0.06362209469079971\n",
      "Epoch: 9 Batch: 55 Loss: 0.10149302333593369\n",
      "Epoch: 9 Batch: 56 Loss: 0.04878835380077362\n",
      "Epoch: 9 Batch: 57 Loss: 0.15146178007125854\n",
      "Epoch: 9 Batch: 58 Loss: 0.18615755438804626\n",
      "Epoch: 9 Batch: 59 Loss: 0.12199448049068451\n",
      "Epoch: 9 Batch: 60 Loss: 0.0872768685221672\n",
      "Epoch: 9 Batch: 61 Loss: 0.1162818968296051\n",
      "Epoch: 9 Batch: 62 Loss: 0.16362598538398743\n",
      "Epoch: 9 Batch: 63 Loss: 0.25997382402420044\n",
      "Epoch: 9 Batch: 64 Loss: 0.11135340481996536\n",
      "Epoch: 9 Batch: 65 Loss: 0.08140718936920166\n",
      "Epoch: 9 Batch: 66 Loss: 0.05722406506538391\n",
      "Epoch: 9 Batch: 67 Loss: 0.15901654958724976\n",
      "Epoch: 9 Batch: 68 Loss: 0.3634033501148224\n",
      "Epoch: 9 Batch: 69 Loss: 0.21727785468101501\n",
      "Epoch: 9 Batch: 70 Loss: 0.03762651979923248\n",
      "Epoch: 9 Batch: 71 Loss: 0.05113508924841881\n",
      "Epoch: 9 Batch: 72 Loss: 0.08480394631624222\n",
      "Epoch: 9 Batch: 73 Loss: 0.3779030740261078\n",
      "Epoch: 9 Batch: 74 Loss: 0.09486877918243408\n",
      "Epoch: 9 Batch: 75 Loss: 0.21013501286506653\n",
      "Epoch: 9 Batch: 76 Loss: 0.1473381072282791\n",
      "Epoch: 9 Batch: 77 Loss: 0.1680677831172943\n",
      "Epoch: 9 Batch: 78 Loss: 0.1201121062040329\n",
      "Epoch: 9 Batch: 79 Loss: 0.15224602818489075\n",
      "Epoch: 9 Batch: 80 Loss: 0.12367002665996552\n",
      "Epoch: 9 Batch: 81 Loss: 0.0739884003996849\n",
      "Epoch: 9 Batch: 82 Loss: 0.13690608739852905\n",
      "Epoch: 9 Batch: 83 Loss: 0.06758441776037216\n",
      "Epoch: 9 Batch: 84 Loss: 0.09213573485612869\n",
      "Epoch: 9 Batch: 85 Loss: 0.09815920144319534\n",
      "Epoch: 9 Batch: 86 Loss: 0.1346667855978012\n",
      "Epoch: 9 Batch: 87 Loss: 0.18357166647911072\n",
      "Epoch: 9 Batch: 88 Loss: 0.1404658704996109\n",
      "Epoch: 9 Batch: 89 Loss: 0.13123014569282532\n",
      "Epoch: 9 Batch: 90 Loss: 0.10261482000350952\n",
      "Epoch: 9 Batch: 91 Loss: 0.07697100937366486\n",
      "Epoch: 9 Batch: 92 Loss: 0.13608187437057495\n",
      "Epoch: 9 Batch: 93 Loss: 0.12998749315738678\n",
      "Epoch: 9 Batch: 94 Loss: 0.2899497449398041\n",
      "Epoch: 9 Batch: 95 Loss: 0.06479685753583908\n",
      "Epoch: 9 Batch: 96 Loss: 0.14013905823230743\n",
      "Epoch: 9 Batch: 97 Loss: 0.09387695789337158\n",
      "Epoch: 9 Batch: 98 Loss: 0.1179836243391037\n",
      "Epoch: 9 Batch: 99 Loss: 0.13513390719890594\n",
      "[0.7007284760475159, 0.6956275701522827, 0.6935051679611206, 0.6897905468940735, 0.6876078844070435, 0.6849812865257263, 0.6819064617156982, 0.6796293258666992, 0.6782955527305603, 0.6732543110847473, 0.6692911386489868, 0.6606659889221191, 0.6590780019760132, 0.6386867761611938, 0.6154899597167969, 0.5813717246055603, 0.5548110604286194, 0.4462732970714569, 0.4377937316894531, 0.3476307690143585, 0.3645615577697754, 0.32811009883880615, 0.21191000938415527, 0.2297157645225525, 0.2908761203289032, 0.13744884729385376, 0.3468904495239258, 0.4022639989852905, 0.22460955381393433, 0.12818577885627747, 0.33754369616508484, 0.2090664952993393, 0.08725742995738983, 0.13546401262283325, 0.10769446194171906, 0.1105031818151474, 0.18205945193767548, 0.1312374472618103, 0.08247056603431702, 0.20883959531784058, 0.13828949630260468, 0.14656615257263184, 0.11402750015258789, 0.4410594701766968, 0.13507206737995148, 0.20085380971431732, 0.05797500163316727, 0.16300781071186066, 0.3001467287540436, 0.143684983253479, 0.06026431918144226, 0.2192194014787674, 0.0973961278796196, 0.20888620615005493, 0.13825082778930664, 0.19465763866901398, 0.09928753972053528, 0.0902165099978447, 0.09823976457118988, 0.26172876358032227, 0.17185735702514648, 0.07592673599720001, 0.18797220289707184, 0.2030920535326004, 0.2117152214050293, 0.126543328166008, 0.2402821034193039, 0.2112313210964203, 0.09902960807085037, 0.14586380124092102, 0.24656814336776733, 0.10663706809282303, 0.20225213468074799, 0.19574865698814392, 0.09126152843236923, 0.09351003915071487, 0.2249106615781784, 0.18685220181941986, 0.16390997171401978, 0.1705106645822525, 0.27819302678108215, 0.11216208338737488, 0.12550856173038483, 0.2358645498752594, 0.08364441990852356, 0.1477406919002533, 0.06688416004180908, 0.31141844391822815, 0.06202306970953941, 0.20307227969169617, 0.13803386688232422, 0.05824797600507736, 0.1889490783214569, 0.25472205877304077, 0.08756447583436966, 0.20463654398918152, 0.08766557276248932, 0.042739737778902054, 0.2670630216598511, 0.11828590929508209, 0.22001923620700836, 0.06719233095645905, 0.11462261527776718, 0.34697818756103516, 0.190263032913208, 0.06342712044715881, 0.060184840112924576, 0.08498510718345642, 0.03924142196774483, 0.0344000943005085, 0.25113019347190857, 0.1304735392332077, 0.21408513188362122, 0.033503297716379166, 0.14316098392009735, 0.24334387481212616, 0.1076240986585617, 0.1422860324382782, 0.18575242161750793, 0.07758843153715134, 0.12331872433423996, 0.2600939869880676, 0.09784054011106491, 0.13872350752353668, 0.10244376212358475, 0.19282349944114685, 0.16044539213180542, 0.09173297137022018, 0.24799126386642456, 0.10823310911655426, 0.08732704818248749, 0.24498841166496277, 0.22010841965675354, 0.12726520001888275, 0.14767521619796753, 0.21604427695274353, 0.21070054173469543, 0.1497790664434433, 0.13025504350662231, 0.06580238789319992, 0.22182278335094452, 0.1497671902179718, 0.10779355466365814, 0.3353461027145386, 0.1382196694612503, 0.17127439379692078, 0.1635037511587143, 0.12372590601444244, 0.06787697970867157, 0.2673555016517639, 0.14317387342453003, 0.20732134580612183, 0.14918164908885956, 0.16027891635894775, 0.08442671597003937, 0.11983190476894379, 0.19337889552116394, 0.11280917376279831, 0.13953542709350586, 0.12622669339179993, 0.12132992595434189, 0.11567916721105576, 0.12170115858316422, 0.17016473412513733, 0.0971844270825386, 0.06042061746120453, 0.20529448986053467, 0.11183622479438782, 0.067411869764328, 0.08906055986881256, 0.24532920122146606, 0.05163965001702309, 0.216626837849617, 0.0975480005145073, 0.1890859454870224, 0.1765287220478058, 0.1374969780445099, 0.2530663013458252, 0.12871213257312775, 0.1472216695547104, 0.1602294147014618, 0.1066078469157219, 0.2625919282436371, 0.11969810724258423, 0.24520795047283173, 0.05929534137248993, 0.23670342564582825, 0.2739587724208832, 0.14133179187774658, 0.16707785427570343, 0.05641930550336838, 0.07426901161670685, 0.11515690386295319, 0.09848146140575409, 0.1111358106136322, 0.05894600600004196, 0.03662628307938576, 0.10285598039627075, 0.161324605345726, 0.05211801826953888, 0.03616858273744583, 0.12327396869659424, 0.04377856105566025, 0.27983957529067993, 0.14495430886745453, 0.11944787949323654, 0.22274723649024963, 0.1681867092847824, 0.19401004910469055, 0.17373806238174438, 0.16369792819023132, 0.20111167430877686, 0.10613755881786346, 0.07941445708274841, 0.06404054909944534, 0.06509777903556824, 0.26778632402420044, 0.18907566368579865, 0.1575857251882553, 0.0985007956624031, 0.09276917576789856, 0.1441819965839386, 0.12024436146020889, 0.3544006049633026, 0.06390734761953354, 0.12771821022033691, 0.05810800567269325, 0.12340602278709412, 0.06077958270907402, 0.0744657889008522, 0.11729507893323898, 0.11626124382019043, 0.08961012959480286, 0.2933976352214813, 0.039762578904628754, 0.19426561892032623, 0.04920722171664238, 0.06981144845485687, 0.05144340544939041, 0.06739316880702972, 0.10515697300434113, 0.08797105401754379, 0.22662022709846497, 0.09386177361011505, 0.08305288106203079, 0.1473608762025833, 0.0841294676065445, 0.3542425334453583, 0.16088953614234924, 0.10775856673717499, 0.08294176310300827, 0.11741535365581512, 0.15240883827209473, 0.08269140869379044, 0.1016138568520546, 0.14352773129940033, 0.12485149502754211, 0.10779968649148941, 0.06788165867328644, 0.15890921652317047, 0.13901814818382263, 0.03793249651789665, 0.1593993604183197, 0.35699519515037537, 0.12008368223905563, 0.17375753819942474, 0.1451105773448944, 0.12095450609922409, 0.13398298621177673, 0.1598864495754242, 0.08463543653488159, 0.17737188935279846, 0.18061023950576782, 0.40865930914878845, 0.21111485362052917, 0.08937575668096542, 0.14388008415699005, 0.12347117811441422, 0.03327757492661476, 0.10070370137691498, 0.14358174800872803, 0.27159619331359863, 0.11776447296142578, 0.020900746807456017, 0.2605842649936676, 0.29471632838249207, 0.13919030129909515, 0.14754195511341095, 0.12435218691825867, 0.06697322428226471, 0.14395882189273834, 0.10096526890993118, 0.07928869873285294, 0.13304346799850464, 0.18569371104240417, 0.2669036388397217, 0.07342200726270676, 0.060256943106651306, 0.11515697091817856, 0.12758782505989075, 0.1000455766916275, 0.06672203540802002, 0.09338375180959702, 0.12540313601493835, 0.149126335978508, 0.04445384442806244, 0.15516921877861023, 0.05899622291326523, 0.18475231528282166, 0.03494606167078018, 0.11938582360744476, 0.4017569422721863, 0.046430639922618866, 0.1476328819990158, 0.19745704531669617, 0.13533130288124084, 0.06406573206186295, 0.23191112279891968, 0.0910661593079567, 0.13571685552597046, 0.09331690520048141, 0.127145916223526, 0.2117774784564972, 0.03516451269388199, 0.14562810957431793, 0.18102288246154785, 0.07728159427642822, 0.1243593618273735, 0.10241621732711792, 0.09643567353487015, 0.056230057030916214, 0.15756723284721375, 0.13775822520256042, 0.10487103462219238, 0.15079836547374725, 0.15190261602401733, 0.26302245259284973, 0.09649787098169327, 0.13483533263206482, 0.11046838760375977, 0.030531838536262512, 0.1277916133403778, 0.14533644914627075, 0.21500231325626373, 0.11186549067497253, 0.2887619137763977, 0.10003040730953217, 0.040880702435970306, 0.1337105631828308, 0.09805052727460861, 0.09480424970388412, 0.1276792287826538, 0.1278221309185028, 0.04031568020582199, 0.1241426169872284, 0.10405287891626358, 0.06508062779903412, 0.12533468008041382, 0.09969852864742279, 0.14897730946540833, 0.25228357315063477, 0.1965273916721344, 0.07079429924488068, 0.16408458352088928, 0.23265117406845093, 0.06791052967309952, 0.3467704653739929, 0.0696435496211052, 0.1494240015745163, 0.16689729690551758, 0.08394616842269897, 0.19840028882026672, 0.0353131964802742, 0.07498718053102493, 0.13209225237369537, 0.08791621774435043, 0.22408242523670197, 0.28586581349372864, 0.08693158626556396, 0.08282576501369476, 0.21958553791046143, 0.07402178645133972, 0.18452416360378265, 0.14384202659130096, 0.0727791115641594, 0.09372184425592422, 0.1547754555940628, 0.10753407329320908, 0.2072448432445526, 0.0719519779086113, 0.12701477110385895, 0.09048455953598022, 0.264243483543396, 0.07139788568019867, 0.1858631819486618, 0.17931821942329407, 0.14267423748970032, 0.11928173899650574, 0.08219204097986221, 0.0871158093214035, 0.2769218385219574, 0.061857789754867554, 0.17842626571655273, 0.16549266874790192, 0.05606447905302048, 0.35409870743751526, 0.09918604791164398, 0.18631528317928314, 0.12367518991231918, 0.1072096899151802, 0.11275732517242432, 0.08097909390926361, 0.02063298225402832, 0.15459859371185303, 0.255127489566803, 0.1668984293937683, 0.0980457067489624, 0.20879359543323517, 0.08875404298305511, 0.16189217567443848, 0.2241348773241043, 0.10607502609491348, 0.1297455132007599, 0.07681471854448318, 0.13116872310638428, 0.1741163283586502, 0.14644217491149902, 0.09006215631961823, 0.1353597491979599, 0.20808905363082886, 0.08224045485258102, 0.29158103466033936, 0.3146210312843323, 0.052571918815374374, 0.14910487830638885, 0.06071775406599045, 0.028298718854784966, 0.09644484519958496, 0.13747230172157288, 0.34940844774246216, 0.09554187208414078, 0.07547368854284286, 0.16890650987625122, 0.12003414332866669, 0.0854753777384758, 0.14228375256061554, 0.15357890725135803, 0.17562048137187958, 0.051840465515851974, 0.04053312912583351, 0.09162028133869171, 0.14705567061901093, 0.03320028632879257, 0.1184152364730835, 0.3242686986923218, 0.18894101679325104, 0.147386372089386, 0.18178951740264893, 0.09351884573698044, 0.12579284608364105, 0.30643928050994873, 0.09300829470157623, 0.03060689941048622, 0.2711515724658966, 0.1309821605682373, 0.1506853550672531, 0.06940910220146179, 0.03958243876695633, 0.05483785644173622, 0.04318802058696747, 0.06085631251335144, 0.1289791464805603, 0.04323766008019447, 0.05858336761593819, 0.0724930465221405, 0.035151269286870956, 0.29722702503204346, 0.09207271039485931, 0.08511019498109818, 0.10509607195854187, 0.3035096824169159, 0.12029188126325607, 0.15391968190670013, 0.1488470435142517, 0.08335772901773453, 0.08870896697044373, 0.10814187675714493, 0.09120102226734161, 0.18022622168064117, 0.09094768017530441, 0.044839609414339066, 0.11267455667257309, 0.08141489326953888, 0.148543119430542, 0.05495380610227585, 0.06660699844360352, 0.21570822596549988, 0.10625959932804108, 0.06966488808393478, 0.10089462250471115, 0.05771886557340622, 0.08912352472543716, 0.3291652798652649, 0.04975111410021782, 0.0670379027724266, 0.1384412944316864, 0.14875970780849457, 0.08036550879478455, 0.28781190514564514, 0.053335051983594894, 0.04056764021515846, 0.03424648940563202, 0.15777041018009186, 0.16185572743415833, 0.13421179354190826, 0.14457501471042633, 0.1260685920715332, 0.13539449870586395, 0.08883101493120193, 0.09953556209802628, 0.18783704936504364, 0.1498470902442932, 0.06951547414064407, 0.06596693396568298, 0.042334482073783875, 0.13226768374443054, 0.09402191638946533, 0.13471761345863342, 0.4190129041671753, 0.14305151998996735, 0.16137714684009552, 0.10711128264665604, 0.07051245123147964, 0.057058967649936676, 0.16740640997886658, 0.17425169050693512, 0.1040760800242424, 0.05062372609972954, 0.07594958692789078, 0.11770714074373245, 0.12934942543506622, 0.08818629384040833, 0.046613406389951706, 0.3669030964374542, 0.10527205467224121, 0.1260160207748413, 0.13565434515476227, 0.06969311833381653, 0.09355450421571732, 0.2044650763273239, 0.1586286574602127, 0.16732065379619598, 0.09722521156072617, 0.07515392452478409, 0.0734601765871048, 0.046407025307416916, 0.10979928076267242, 0.36133456230163574, 0.11052779853343964, 0.10825130343437195, 0.22979094088077545, 0.13737301528453827, 0.12388499081134796, 0.023792410269379616, 0.12174874544143677, 0.15588733553886414, 0.12434116750955582, 0.08100602775812149, 0.15047329664230347, 0.05619990453124046, 0.07094506919384003, 0.13817203044891357, 0.43497759103775024, 0.08489128947257996, 0.029685713350772858, 0.18596044182777405, 0.18425650894641876, 0.18483629822731018, 0.156373992562294, 0.23859208822250366, 0.33367615938186646, 0.16122961044311523, 0.14931994676589966, 0.15297846496105194, 0.02229457162320614, 0.05251095071434975, 0.28675708174705505, 0.04413825273513794, 0.07230497896671295, 0.051425449550151825, 0.11814072728157043, 0.12123668193817139, 0.13477307558059692, 0.04942319542169571, 0.16896581649780273, 0.09905841201543808, 0.03591611981391907, 0.06650715321302414, 0.1134946197271347, 0.10354328155517578, 0.01934661529958248, 0.11209867894649506, 0.08427946269512177, 0.07203961908817291, 0.11571759730577469, 0.08319962024688721, 0.17800264060497284, 0.31518658995628357, 0.07368176430463791, 0.13905051350593567, 0.10376585274934769, 0.061406318098306656, 0.14447781443595886, 0.10719359666109085, 0.08023668080568314, 0.18304428458213806, 0.10952650755643845, 0.14086443185806274, 0.14355795085430145, 0.06826402992010117, 0.06825555860996246, 0.08423691987991333, 0.10556916892528534, 0.13832587003707886, 0.03728658705949783, 0.08741767704486847, 0.04353044927120209, 0.04200940206646919, 0.17008350789546967, 0.30882951617240906, 0.05046558007597923, 0.15819963812828064, 0.09195055812597275, 0.03327106684446335, 0.170924112200737, 0.08556503057479858, 0.08688078075647354, 0.1480761468410492, 0.10723257064819336, 0.0875072181224823, 0.42121344804763794, 0.1178864911198616, 0.21742486953735352, 0.0854680985212326, 0.2260502576828003, 0.06758543848991394, 0.1850491315126419, 0.06342902779579163, 0.122213214635849, 0.0808025673031807, 0.1155448779463768, 0.16928978264331818, 0.09737178683280945, 0.061635129153728485, 0.09853501617908478, 0.12129386514425278, 0.14161477982997894, 0.04605334997177124, 0.07939551025629044, 0.19820402562618256, 0.06294956803321838, 0.14545343816280365, 0.15664538741111755, 0.07320529967546463, 0.10358336567878723, 0.10954228043556213, 0.14329597353935242, 0.01262580044567585, 0.06925444304943085, 0.07579690963029861, 0.1918889284133911, 0.0735725462436676, 0.04200507700443268, 0.03349993750452995, 0.27762797474861145, 0.2306707799434662, 0.24957206845283508, 0.07536487281322479, 0.1396297812461853, 0.11361492425203323, 0.23101654648780823, 0.14293284714221954, 0.08836041390895844, 0.1231982558965683, 0.05074894800782204, 0.11985711008310318, 0.13813552260398865, 0.0983574315905571, 0.12743327021598816, 0.0264405719935894, 0.4883945882320404, 0.16847729682922363, 0.11138305068016052, 0.10015131533145905, 0.028588570654392242, 0.24338391423225403, 0.13015735149383545, 0.12945464253425598, 0.3742462694644928, 0.1371244490146637, 0.12915284931659698, 0.13929812610149384, 0.05836813896894455, 0.0933670848608017, 0.0275342408567667, 0.09106491506099701, 0.138162299990654, 0.2060135006904602, 0.1783207803964615, 0.039906494319438934, 0.20820149779319763, 0.09212659299373627, 0.06947673857212067, 0.15794995427131653, 0.07790865749120712, 0.12468963861465454, 0.1540575921535492, 0.10011579096317291, 0.2171737253665924, 0.16384388506412506, 0.11036354303359985, 0.02295355312526226, 0.4003969430923462, 0.1836216151714325, 0.08096473664045334, 0.12708322703838348, 0.1659666746854782, 0.08918260782957077, 0.11842328310012817, 0.09447705745697021, 0.12933997809886932, 0.1395723670721054, 0.10274184495210648, 0.048214416950941086, 0.1327497363090515, 0.22461555898189545, 0.08255685120820999, 0.09964781999588013, 0.14592450857162476, 0.3072412610054016, 0.12105704843997955, 0.05215775966644287, 0.0790434405207634, 0.04552248492836952, 0.10531176626682281, 0.03818801790475845, 0.003986472729593515, 0.09215925633907318, 0.2241780310869217, 0.4595670700073242, 0.05598603934049606, 0.09570136666297913, 0.3750307857990265, 0.09521268308162689, 0.13935653865337372, 0.08181940019130707, 0.08510036766529083, 0.05794120952486992, 0.18562310934066772, 0.11056581139564514, 0.2231670618057251, 0.08715575933456421, 0.11603591591119766, 0.23484598100185394, 0.13361182808876038, 0.09813595563173294, 0.11198265105485916, 0.034815460443496704, 0.15520739555358887, 0.2871265113353729, 0.04646120220422745, 0.12142432481050491, 0.10471764206886292, 0.05596425011754036, 0.06709069758653641, 0.09573284536600113, 0.05563822016119957, 0.10692163556814194, 0.05405494198203087, 0.06654605269432068, 0.11795197427272797, 0.08254396915435791, 0.05059319734573364, 0.24809220433235168, 0.20342537760734558, 0.14343084394931793, 0.23804199695587158, 0.11474675685167313, 0.1179409995675087, 0.23471027612686157, 0.0798528864979744, 0.062313180416822433, 0.21776831150054932, 0.06584572046995163, 0.0337790846824646, 0.09418794512748718, 0.11427851021289825, 0.3977580964565277, 0.040462054312229156, 0.11625200510025024, 0.2930304706096649, 0.12269960343837738, 0.1844744086265564, 0.13930293917655945, 0.11188431084156036, 0.09310353547334671, 0.07645541429519653, 0.12124975770711899, 0.12088178843259811, 0.07168743759393692, 0.14346523582935333, 0.08639797568321228, 0.025253083556890488, 0.0944671481847763, 0.09010927379131317, 0.1264672577381134, 0.1313154399394989, 0.026537613943219185, 0.07121247053146362, 0.06770524382591248, 0.0985490083694458, 0.11147412657737732, 0.21091891825199127, 0.08768878132104874, 0.2173386812210083, 0.3737730383872986, 0.10812150686979294, 0.025083251297473907, 0.07581257820129395, 0.13978826999664307, 0.03278619050979614, 0.16978593170642853, 0.09536683559417725, 0.12664136290550232, 0.09412579238414764, 0.1874660849571228, 0.11226727068424225, 0.09951139241456985, 0.13511231541633606, 0.10593706369400024, 0.1493409276008606, 0.19305776059627533, 0.061235908418893814, 0.02336708828806877, 0.17524641752243042, 0.1257735639810562, 0.1977313756942749, 0.20576795935630798, 0.0722736194729805, 0.1882244050502777, 0.17077448964118958, 0.009956001304090023, 0.04764311760663986, 0.049395602196455, 0.05667858570814133, 0.04529833793640137, 0.21347777545452118, 0.04684038087725639, 0.11793236434459686, 0.09905171394348145, 0.12212644517421722, 0.04196815565228462, 0.14376375079154968, 0.4118242859840393, 0.03694290295243263, 0.16366156935691833, 0.08114346861839294, 0.21699370443820953, 0.1103430837392807, 0.14051169157028198, 0.12787480652332306, 0.09105907380580902, 0.10340113937854767, 0.07429302483797073, 0.0379774384200573, 0.025233885273337364, 0.11982832849025726, 0.22903650999069214, 0.0905148983001709, 0.2715688645839691, 0.08043595403432846, 0.4411388039588928, 0.43791747093200684, 0.12986081838607788, 0.10194619745016098, 0.03275713697075844, 0.15401829779148102, 0.1292385756969452, 0.05542587488889694, 0.15252314507961273, 0.2014860212802887, 0.1072206199169159, 0.13717299699783325, 0.09137510508298874, 0.06765604019165039, 0.1569620966911316, 0.1991681456565857, 0.09496248513460159, 0.06767892837524414, 0.07987793534994125, 0.05581151321530342, 0.41752034425735474, 0.028876157477498055, 0.21686270833015442, 0.08423329889774323, 0.26574012637138367, 0.13018237054347992, 0.19074347615242004, 0.25809621810913086, 0.06614284217357635, 0.07570204883813858, 0.3412284553050995, 0.19050860404968262, 0.02824498899281025, 0.09133695811033249, 0.0518731065094471, 0.037215325981378555, 0.18516094982624054, 0.04024709388613701, 0.1362016499042511, 0.03649255633354187, 0.04579636827111244, 0.09148449450731277, 0.13876836001873016, 0.09098145365715027, 0.09845271706581116, 0.018759464845061302, 0.09222407639026642, 0.10172484815120697, 0.13414998352527618, 0.03642772510647774, 0.06904913485050201, 0.08537165820598602, 0.06979013979434967, 0.08247876167297363, 0.19795218110084534, 0.10610831528902054, 0.04576529562473297, 0.24565961956977844, 0.11206035315990448, 0.043493397533893585, 0.1363455206155777, 0.04946700483560562, 0.1106119453907013, 0.07780960947275162, 0.10408651828765869, 0.22934970259666443, 0.31927379965782166, 0.09772209823131561, 0.06420336663722992, 0.08995822072029114, 0.030829349532723427, 0.1444762945175171, 0.11651355028152466, 0.06362209469079971, 0.10149302333593369, 0.04878835380077362, 0.15146178007125854, 0.18615755438804626, 0.12199448049068451, 0.0872768685221672, 0.1162818968296051, 0.16362598538398743, 0.25997382402420044, 0.11135340481996536, 0.08140718936920166, 0.05722406506538391, 0.15901654958724976, 0.3634033501148224, 0.21727785468101501, 0.03762651979923248, 0.05113508924841881, 0.08480394631624222, 0.3779030740261078, 0.09486877918243408, 0.21013501286506653, 0.1473381072282791, 0.1680677831172943, 0.1201121062040329, 0.15224602818489075, 0.12367002665996552, 0.0739884003996849, 0.13690608739852905, 0.06758441776037216, 0.09213573485612869, 0.09815920144319534, 0.1346667855978012, 0.18357166647911072, 0.1404658704996109, 0.13123014569282532, 0.10261482000350952, 0.07697100937366486, 0.13608187437057495, 0.12998749315738678, 0.2899497449398041, 0.06479685753583908, 0.14013905823230743, 0.09387695789337158, 0.1179836243391037, 0.13513390719890594]\n",
      "[0.1534375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9375, 0.9378125, 0.938125, 0.9375]\n",
      "[[...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...], [...]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epoch_num = 10\n",
    "\n",
    "model = skyModel()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_values = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for i in range(epoch_num):\n",
    "    for j, (x, y_true) in enumerate(dl_train):\n",
    "        y_pred = model(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "        l = loss.item()\n",
    "        loss_values.append(l)\n",
    "        print(\"Epoch:\", i, \"Batch:\", j, \"Loss:\", l)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if j%50 == 0:\n",
    "            train_acc = eval_acc(model, dl_train)\n",
    "            test_acc = eval_acc(model, dl_test)\n",
    "            print(\"Epoch:\", i, \"Batch:\", j)\n",
    "            print(\"Train acc:\", train_acc)\n",
    "            print(\"Test acc:\", test_acc)\n",
    "            print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_accs)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"skyModel.bin\")\n",
    "print(loss_values)\n",
    "print(train_accs)\n",
    "print(test_accs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
